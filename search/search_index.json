{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"fundamentals/feature-engineering/","title":"Feature Engineering for Hydrological Models","text":""},{"location":"fundamentals/feature-engineering/#overview","title":"\ud83c\udfaf Overview","text":"<p>Feature engineering is the process of creating new input variables (features) from existing data to improve model performance. In hydrology, this often involves capturing temporal dependencies and seasonal patterns.</p>"},{"location":"fundamentals/feature-engineering/#why-feature-engineering-matters-in-hydrology","title":"\ud83c\udf0a Why Feature Engineering Matters in Hydrology","text":"<p>Hydrological systems have memory: - Rainfall today doesn't immediately become runoff - Soil moisture affects future discharge - Groundwater responds slowly to precipitation - Temperature influences evapotranspiration with delays</p> <p>By engineering features that capture these relationships, we can significantly improve model predictions.</p>"},{"location":"fundamentals/feature-engineering/#cross-correlation-analysis","title":"\ud83d\udcca Cross-Correlation Analysis","text":""},{"location":"fundamentals/feature-engineering/#understanding-cross-correlation-function-ccf","title":"Understanding Cross-Correlation Function (CCF)","text":"<p>The CCF measures how past values of one variable relate to current values of another.</p>"},{"location":"fundamentals/feature-engineering/#mathematical-formula","title":"Mathematical Formula","text":"\\[ \\text{CCF}(k) = \\frac{\\sum_{t} (x_{t+k} - \\bar{x})(y_t - \\bar{y})}{\\sqrt{\\sum_{t} (x_{t+k} - \\bar{x})^2 \\sum_{t} (y_t - \\bar{y})^2}} \\] <p>Where: - \\(x_t\\) = Independent variable (e.g., rainfall) at time \\(t\\) - \\(y_t\\) = Dependent variable (e.g., discharge) at time \\(t\\) - \\(k\\) = Lag value - \\(\\bar{x}\\), \\(\\bar{y}\\) = Means of respective variables</p>"},{"location":"fundamentals/feature-engineering/#significance-threshold","title":"Significance Threshold","text":"\\[ \\text{Significance Limit} = \\pm \\frac{1.96}{\\sqrt{N}} \\] <p>Where \\(N\\) is the number of observations.</p>"},{"location":"fundamentals/feature-engineering/#implementation","title":"Implementation","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_ccf\n\ndef analyze_cross_correlation(df, max_lags=12):\n    \"\"\"\n    Compute and plot cross-correlation between inputs and discharge\n\n    Parameters:\n    -----------\n    df : DataFrame\n        Data with columns for inputs and 'Discharge'\n    max_lags : int\n        Maximum number of lags to analyze\n\n    Returns:\n    --------\n    dict : Significant lags for each variable\n    \"\"\"\n    # Separate inputs and output\n    inputs = df.drop('Discharge', axis=1)\n    output = df['Discharge']\n\n    # Store significant lags\n    significant_lags = {}\n\n    # Create subplots\n    fig, axes = plt.subplots(len(inputs.columns), 1, \n                             figsize=(10, 3*len(inputs.columns)))\n\n    if len(inputs.columns) == 1:\n        axes = [axes]\n\n    # Analyze each input variable\n    for i, col in enumerate(inputs.columns):\n        # Remove mean (required for CCF)\n        var1 = inputs[col] - inputs[col].mean()\n        var2 = output - output.mean()\n\n        # Plot CCF\n        plot_ccf(var1, var2, lags=max_lags, ax=axes[i])\n        axes[i].set_title(f'CCF: {col} vs Discharge')\n        axes[i].set_xlabel('Lag (days)')\n        axes[i].set_ylabel('Correlation')\n\n        # Find significant lags (simplified approach)\n        n = len(var1)\n        significance_level = 1.96 / np.sqrt(n)\n\n        # You can extract significant lags programmatically here\n        # For now, we'll note them visually from the plot\n\n    plt.tight_layout()\n    plt.show()\n\n    return significant_lags\n\n# Example usage\ndf = pd.read_csv('Discharge_30years.csv', parse_dates=['Date'], index_col='Date')\nanalyze_cross_correlation(df)\n</code></pre>"},{"location":"fundamentals/feature-engineering/#lag-features","title":"\ud83d\udd04 Lag Features","text":""},{"location":"fundamentals/feature-engineering/#what-are-lag-features","title":"What are Lag Features?","text":"<p>Lag features are past values of variables used as inputs. They help models \"remember\" previous conditions.</p>"},{"location":"fundamentals/feature-engineering/#example-creating-lag-features","title":"Example: Creating Lag Features","text":"Date Rainfall Rainfall_lag1 Rainfall_lag2 Rainfall_lag3 Day 1 10.5 NaN NaN NaN Day 2 5.2 10.5 NaN NaN Day 3 0.0 5.2 10.5 NaN Day 4 15.3 0.0 5.2 10.5 Day 5 8.7 15.3 0.0 5.2"},{"location":"fundamentals/feature-engineering/#implementation_1","title":"Implementation","text":"<pre><code>def create_lag_features(df, variable_lags):\n    \"\"\"\n    Create lagged features for specified variables\n\n    Parameters:\n    -----------\n    df : DataFrame\n        Original data\n    variable_lags : dict\n        Dictionary mapping variable names to list of lag values\n        Example: {'Rainfall': [1, 2, 3], 'Tmax': [1, 2]}\n\n    Returns:\n    --------\n    DataFrame : Data with added lag features\n    \"\"\"\n    df_features = df.copy()\n\n    for variable, lags in variable_lags.items():\n        for lag in lags:\n            df_features[f'{variable}_lag{lag}'] = df[variable].shift(lag)\n\n    return df_features\n\n# Example usage\ndf = pd.read_csv('Discharge_30years.csv', parse_dates=['Date'], index_col='Date')\n\n# Define lags based on CCF analysis\nlag_config = {\n    'Rainfall': [1, 2, 3],\n    'Tmax': [1, 2, 3],\n    'Tmin': [1, 2, 3]\n}\n\n# Create features\ndf_with_lags = create_lag_features(df, lag_config)\n\n# Remove rows with NaN (from lagging)\ndf_clean = df_with_lags.dropna()\n\nprint(f\"Original shape: {df.shape}\")\nprint(f\"After adding lags: {df_with_lags.shape}\")\nprint(f\"After removing NaN: {df_clean.shape}\")\nprint(\"\\nNew features created:\")\nprint([col for col in df_with_lags.columns if 'lag' in col])\n</code></pre>"},{"location":"fundamentals/feature-engineering/#rolling-statistics","title":"\ud83d\udcc8 Rolling Statistics","text":""},{"location":"fundamentals/feature-engineering/#moving-averages-and-sums","title":"Moving Averages and Sums","text":"<p>Rolling statistics smooth out short-term fluctuations and highlight longer-term trends.</p> <pre><code>def add_rolling_features(df, windows=[3, 7, 14, 30]):\n    \"\"\"\n    Add rolling statistics as features\n\n    Parameters:\n    -----------\n    df : DataFrame\n        Original data\n    windows : list\n        Window sizes for rolling calculations\n\n    Returns:\n    --------\n    DataFrame : Data with rolling features\n    \"\"\"\n    df_features = df.copy()\n\n    for window in windows:\n        # Rolling mean\n        df_features[f'Rainfall_mean_{window}d'] = (\n            df['Rainfall'].rolling(window=window, min_periods=1).mean()\n        )\n\n        # Rolling sum (cumulative rainfall)\n        df_features[f'Rainfall_sum_{window}d'] = (\n            df['Rainfall'].rolling(window=window, min_periods=1).sum()\n        )\n\n        # Rolling max temperature\n        df_features[f'Tmax_max_{window}d'] = (\n            df['Tmax'].rolling(window=window, min_periods=1).max()\n        )\n\n        # Rolling standard deviation (variability)\n        df_features[f'Rainfall_std_{window}d'] = (\n            df['Rainfall'].rolling(window=window, min_periods=1).std()\n        )\n\n    return df_features\n\n# Example usage\ndf_rolling = add_rolling_features(df)\n</code></pre>"},{"location":"fundamentals/feature-engineering/#temporal-features","title":"\ud83d\uddd3\ufe0f Temporal Features","text":""},{"location":"fundamentals/feature-engineering/#extracting-time-based-information","title":"Extracting Time-Based Information","text":"<pre><code>def add_temporal_features(df):\n    \"\"\"\n    Add temporal features from date index\n\n    Parameters:\n    -----------\n    df : DataFrame\n        Data with DatetimeIndex\n\n    Returns:\n    --------\n    DataFrame : Data with temporal features\n    \"\"\"\n    df_features = df.copy()\n\n    # Basic temporal features\n    df_features['day_of_year'] = df.index.dayofyear\n    df_features['month'] = df.index.month\n    df_features['quarter'] = df.index.quarter\n    df_features['week_of_year'] = df.index.isocalendar().week\n\n    # Cyclical encoding for month (captures seasonality)\n    df_features['month_sin'] = np.sin(2 * np.pi * df.index.month / 12)\n    df_features['month_cos'] = np.cos(2 * np.pi * df.index.month / 12)\n\n    # Cyclical encoding for day of year\n    df_features['day_sin'] = np.sin(2 * np.pi * df.index.dayofyear / 365)\n    df_features['day_cos'] = np.cos(2 * np.pi * df.index.dayofyear / 365)\n\n    # Season indicator\n    seasons = {1: 'Winter', 2: 'Winter', 3: 'Spring', \n               4: 'Spring', 5: 'Spring', 6: 'Summer',\n               7: 'Summer', 8: 'Summer', 9: 'Fall', \n               10: 'Fall', 11: 'Fall', 12: 'Winter'}\n    df_features['season'] = df.index.month.map(seasons)\n\n    # One-hot encode season\n    season_dummies = pd.get_dummies(df_features['season'], prefix='season')\n    df_features = pd.concat([df_features, season_dummies], axis=1)\n\n    return df_features\n\n# Example usage\ndf_temporal = add_temporal_features(df)\n</code></pre>"},{"location":"fundamentals/feature-engineering/#interaction-features","title":"\ud83e\uddee Interaction Features","text":""},{"location":"fundamentals/feature-engineering/#creating-feature-combinations","title":"Creating Feature Combinations","text":"<pre><code>def add_interaction_features(df):\n    \"\"\"\n    Create interaction features between variables\n    \"\"\"\n    df_features = df.copy()\n\n    # Rainfall-Temperature interaction\n    df_features['Rain_Temp_interaction'] = df['Rainfall'] * df['Tmax']\n\n    # Temperature range\n    df_features['Temp_range'] = df['Tmax'] - df['Tmin']\n\n    # Antecedent Precipitation Index (API)\n    # Weighted sum of past rainfall\n    weights = np.array([0.5, 0.3, 0.2])  # Decreasing weights\n    for i, w in enumerate(weights, 1):\n        if f'Rainfall_lag{i}' in df_features.columns:\n            if i == 1:\n                df_features['API'] = w * df_features[f'Rainfall_lag{i}']\n            else:\n                df_features['API'] += w * df_features[f'Rainfall_lag{i}']\n\n    return df_features\n</code></pre>"},{"location":"fundamentals/feature-engineering/#domain-specific-features","title":"\ud83d\udd2c Domain-Specific Features","text":""},{"location":"fundamentals/feature-engineering/#hydrological-indices","title":"Hydrological Indices","text":"<pre><code>def add_hydrological_features(df):\n    \"\"\"\n    Add hydrology-specific features\n    \"\"\"\n    df_features = df.copy()\n\n    # Baseflow index (using simple filter)\n    # This is a simplified approach\n    alpha = 0.925  # Filter parameter\n    baseflow = [df['Discharge'].iloc[0]]\n\n    for i in range(1, len(df)):\n        bf = alpha * baseflow[-1] + (1 - alpha) * df['Discharge'].iloc[i]\n        baseflow.append(min(bf, df['Discharge'].iloc[i]))\n\n    df_features['Baseflow'] = baseflow\n    df_features['Quickflow'] = df['Discharge'] - df_features['Baseflow']\n\n    # Antecedent discharge (previous day's discharge)\n    df_features['Discharge_lag1'] = df['Discharge'].shift(1)\n\n    # Rate of change in discharge\n    df_features['Discharge_change'] = df['Discharge'].diff()\n\n    # Cumulative rainfall over season\n    df_features['Cumulative_rainfall'] = df.groupby(df.index.year)['Rainfall'].cumsum()\n\n    return df_features\n</code></pre>"},{"location":"fundamentals/feature-engineering/#complete-feature-engineering-pipeline","title":"\ud83d\udcca Complete Feature Engineering Pipeline","text":"<pre><code>def complete_feature_engineering(df):\n    \"\"\"\n    Complete feature engineering pipeline for hydrological data\n    \"\"\"\n    print(\"Starting feature engineering...\")\n    print(f\"Original shape: {df.shape}\")\n\n    # Step 1: Create lag features\n    lag_config = {\n        'Rainfall': [1, 2, 3],\n        'Tmax': [1, 2],\n        'Tmin': [1, 2],\n        'Discharge': [1, 2]  # Auto-regressive component\n    }\n    df_features = create_lag_features(df, lag_config)\n    print(f\"After lag features: {df_features.shape}\")\n\n    # Step 2: Add rolling statistics\n    df_features = add_rolling_features(df_features, windows=[3, 7, 14])\n    print(f\"After rolling features: {df_features.shape}\")\n\n    # Step 3: Add temporal features\n    df_features = add_temporal_features(df_features)\n    print(f\"After temporal features: {df_features.shape}\")\n\n    # Step 4: Add interaction features\n    df_features = add_interaction_features(df_features)\n    print(f\"After interaction features: {df_features.shape}\")\n\n    # Step 5: Add hydrological features\n    df_features = add_hydrological_features(df_features)\n    print(f\"After hydrological features: {df_features.shape}\")\n\n    # Step 6: Remove rows with NaN\n    df_clean = df_features.dropna()\n    print(f\"After removing NaN: {df_clean.shape}\")\n\n    # Step 7: Separate features and target\n    target = df_clean['Discharge']\n    features = df_clean.drop(['Discharge', 'season'], axis=1)  # Remove non-numeric\n\n    print(f\"\\nFinal features shape: {features.shape}\")\n    print(f\"Final target shape: {target.shape}\")\n    print(f\"\\nTotal features created: {len(features.columns)}\")\n\n    return features, target\n\n# Run the complete pipeline\nfeatures, target = complete_feature_engineering(df)\n</code></pre>"},{"location":"fundamentals/feature-engineering/#feature-selection","title":"\u2705 Feature Selection","text":""},{"location":"fundamentals/feature-engineering/#importance-based-selection","title":"Importance-Based Selection","text":"<pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\ndef select_important_features(features, target, n_features=20):\n    \"\"\"\n    Select most important features using Random Forest\n    \"\"\"\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        features, target, test_size=0.2, shuffle=False\n    )\n\n    # Train Random Forest\n    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n    rf.fit(X_train, y_train)\n\n    # Get feature importances\n    importances = pd.DataFrame({\n        'feature': features.columns,\n        'importance': rf.feature_importances_\n    }).sort_values('importance', ascending=False)\n\n    # Plot top features\n    plt.figure(figsize=(10, 6))\n    plt.barh(importances['feature'][:n_features], \n             importances['importance'][:n_features])\n    plt.xlabel('Importance')\n    plt.title(f'Top {n_features} Most Important Features')\n    plt.gca().invert_yaxis()\n    plt.tight_layout()\n    plt.show()\n\n    # Return top features\n    top_features = importances['feature'][:n_features].tolist()\n    return top_features, importances\n\n# Select top features\ntop_features, importance_df = select_important_features(features, target)\n</code></pre>"},{"location":"fundamentals/feature-engineering/#best-practices","title":"\ud83c\udfaf Best Practices","text":"<ol> <li>Start Simple: Begin with basic lag features before complex transformations</li> <li>Domain Knowledge: Use hydrological understanding to guide feature creation</li> <li>Avoid Leakage: Don't use future information in features</li> <li>Handle Missing Data: Decide strategy before creating features</li> <li>Scale Features: Normalize/standardize for neural networks</li> <li>Validate Impact: Test if new features actually improve performance</li> <li>Document Features: Keep track of what each feature represents</li> </ol>"},{"location":"fundamentals/feature-engineering/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<p>Avoid These Mistakes</p> <ul> <li>Data Leakage: Using future values to predict past (shuffle=False for time series!)</li> <li>Too Many Features: Can lead to overfitting (curse of dimensionality)</li> <li>Highly Correlated Features: Remove redundant features</li> <li>Not Checking Feature Distributions: Outliers can dominate models</li> <li>Ignoring Physical Constraints: Features should make hydrological sense</li> </ul>"},{"location":"fundamentals/feature-engineering/#feature-visualization","title":"\ud83d\udcca Feature Visualization","text":"<pre><code>def visualize_features(df_features, target_col='Discharge', n_features=6):\n    \"\"\"\n    Visualize relationships between features and target\n    \"\"\"\n    import seaborn as sns\n\n    # Select features to plot\n    feature_cols = [col for col in df_features.columns if col != target_col][:n_features]\n\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n\n    for idx, col in enumerate(feature_cols):\n        axes[idx].scatter(df_features[col], df_features[target_col], \n                         alpha=0.5, s=10)\n        axes[idx].set_xlabel(col)\n        axes[idx].set_ylabel(target_col)\n        axes[idx].set_title(f'{col} vs {target_col}')\n\n        # Add trend line\n        z = np.polyfit(df_features[col].dropna(), \n                      df_features.loc[df_features[col].notna(), target_col], 1)\n        p = np.poly1d(z)\n        axes[idx].plot(df_features[col].sort_values(), \n                      p(df_features[col].sort_values()), \n                      \"r--\", alpha=0.8)\n\n    plt.tight_layout()\n    plt.show()\n\n# Visualize feature relationships\nvisualize_features(df_with_lags)\n</code></pre>"},{"location":"fundamentals/feature-engineering/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>Now that you've mastered feature engineering:</p> <ol> <li>Apply these features to Multiple Linear Regression</li> <li>Use them in Artificial Neural Networks</li> <li>Experiment with different lag configurations</li> <li>Try feature selection techniques</li> </ol>"},{"location":"fundamentals/feature-engineering/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Time Series Feature Engineering</li> <li>Feature Engineering for Machine Learning</li> <li>Hydrological Feature Engineering Papers</li> </ul> <p>:material-arrow-left: Performance Metrics</p> <p>:material-arrow-right: Simple Linear Regression</p>"},{"location":"fundamentals/performance-metrics/","title":"Model Performance Indicators","text":""},{"location":"fundamentals/performance-metrics/#overview","title":"\ud83d\udcca Overview","text":"<p>This guide covers the essential metrics for evaluating hydrological prediction models. Understanding these metrics is crucial for assessing model reliability and comparing different approaches.</p>"},{"location":"fundamentals/performance-metrics/#why-performance-metrics-matter","title":"\ud83c\udfaf Why Performance Metrics Matter","text":"<ul> <li>Model Selection: Choose the best model for your specific application</li> <li>Validation: Ensure your model generalizes well to unseen data</li> <li>Improvement: Identify areas where models need refinement</li> <li>Communication: Report model performance to stakeholders</li> </ul>"},{"location":"fundamentals/performance-metrics/#core-metrics","title":"\ud83d\udcc8 Core Metrics","text":""},{"location":"fundamentals/performance-metrics/#1-coefficient-of-determination-r2","title":"1. Coefficient of Determination (R\u00b2)","text":""},{"location":"fundamentals/performance-metrics/#mathematical-formula","title":"Mathematical Formula","text":"\\[ R^2 = \\left( \\frac{ \\sum_{i=1}^{n} (Q_{i}^{\\mathrm{obs}} - \\overline{Q^{\\mathrm{obs}}})(Q_{i}^{\\mathrm{sim}} - \\overline{Q^{\\mathrm{sim}}}) }{ \\sqrt{ \\sum_{i=1}^{n} (Q_{i}^{\\mathrm{obs}} - \\overline{Q^{\\mathrm{obs}}})^2 } \\cdot \\sqrt{ \\sum_{i=1}^{n} (Q_{i}^{\\mathrm{sim}} - \\overline{Q^{\\mathrm{sim}}})^2 } } \\right)^2 \\] <p>Where: - \\(Q_{i}^{\\mathrm{obs}}\\) = observed value at time step \\(i\\) - \\(Q_{i}^{\\mathrm{sim}}\\) = simulated value at time step \\(i\\) - \\(\\overline{Q^{\\mathrm{obs}}}\\), \\(\\overline{Q^{\\mathrm{sim}}}\\) = means of observed and simulated values - \\(n\\) = number of observations</p>"},{"location":"fundamentals/performance-metrics/#interpretation","title":"Interpretation","text":"R\u00b2 Value Interpretation 1.0 Perfect correlation 0.8-1.0 Very strong relationship 0.6-0.8 Strong relationship 0.4-0.6 Moderate relationship 0.2-0.4 Weak relationship &lt; 0.2 Very weak or no relationship <p>Key Points</p> <ul> <li>R\u00b2 measures the proportion of variance explained by the model</li> <li>Values range from 0 to 1</li> <li>Higher values indicate better linear correlation</li> <li>Does not indicate whether predictions are biased</li> </ul>"},{"location":"fundamentals/performance-metrics/#python-implementation","title":"Python Implementation","text":"<pre><code>import numpy as np\n\ndef calculate_r2(observed, simulated):\n    \"\"\"\n    Calculate R\u00b2 between observed and simulated values\n    \"\"\"\n    # Calculate correlation coefficient\n    r = np.corrcoef(observed, simulated)[0, 1]\n\n    # Square it to get R\u00b2\n    r2 = r ** 2\n\n    return r2\n</code></pre>"},{"location":"fundamentals/performance-metrics/#2-nash-sutcliffe-efficiency-nse","title":"2. Nash-Sutcliffe Efficiency (NSE)","text":""},{"location":"fundamentals/performance-metrics/#mathematical-formula_1","title":"Mathematical Formula","text":"\\[ \\mathrm{NSE} = 1 - \\frac{\\sum_{i=1}^{n} \\left(Q_{i}^{\\mathrm{obs}} - Q_{i}^{\\mathrm{sim}}\\right)^2}{\\sum_{i=1}^{n} \\left(Q_{i}^{\\mathrm{obs}} - \\overline{Q^{\\mathrm{obs}}}\\right)^2} \\]"},{"location":"fundamentals/performance-metrics/#interpretation_1","title":"Interpretation","text":"NSE Value Model Performance 1.0 Perfect model 0.75-1.0 Very good performance 0.65-0.75 Good performance 0.50-0.65 Satisfactory performance 0.0-0.50 Unsatisfactory (but better than mean) &lt; 0 Unacceptable (worse than using mean) <p>Important</p> <p>NSE &lt; 0 means the observed mean is a better predictor than the model!</p>"},{"location":"fundamentals/performance-metrics/#python-implementation_1","title":"Python Implementation","text":"<pre><code>def calculate_nse(observed, simulated):\n    \"\"\"\n    Calculate Nash-Sutcliffe Efficiency\n    \"\"\"\n    # Calculate numerator (sum of squared errors)\n    numerator = np.sum((observed - simulated) ** 2)\n\n    # Calculate denominator (variance of observed)\n    denominator = np.sum((observed - np.mean(observed)) ** 2)\n\n    # Calculate NSE\n    nse = 1 - (numerator / denominator)\n\n    return nse\n</code></pre>"},{"location":"fundamentals/performance-metrics/#3-percent-bias-pbias","title":"3. Percent Bias (PBIAS)","text":""},{"location":"fundamentals/performance-metrics/#mathematical-formula_2","title":"Mathematical Formula","text":"\\[ \\mathrm{PBIAS} = 100 \\times \\frac{ \\sum_{i=1}^{n} (Q_{i}^{\\mathrm{obs}} - Q_{i}^{\\mathrm{sim}}) }{ \\sum_{i=1}^{n} Q_{i}^{\\mathrm{obs}} } \\]"},{"location":"fundamentals/performance-metrics/#interpretation_2","title":"Interpretation","text":"PBIAS Value Interpretation 0% No bias (perfect) &gt; 0% Model underestimates &lt; 0% Model overestimates \u00b15% Very good \u00b110% Good \u00b115% Satisfactory &gt; \u00b125% Unsatisfactory <p>Rule of Thumb</p> <ul> <li>For streamflow: PBIAS &lt; \u00b110% is very good</li> <li>For sediment: PBIAS &lt; \u00b115% is very good</li> <li>For nutrients: PBIAS &lt; \u00b125% is acceptable</li> </ul>"},{"location":"fundamentals/performance-metrics/#python-implementation_2","title":"Python Implementation","text":"<pre><code>def calculate_pbias(observed, simulated):\n    \"\"\"\n    Calculate Percent Bias\n    \"\"\"\n    pbias = 100 * np.sum(observed - simulated) / np.sum(observed)\n    return pbias\n</code></pre>"},{"location":"fundamentals/performance-metrics/#complete-evaluation-function","title":"\ud83d\udd27 Complete Evaluation Function","text":"<p>Here's the comprehensive function used throughout this guide:</p> <pre><code>import numpy as np\n\ndef evaluate_model(obs, sim):\n    \"\"\"\n    Calculate R\u00b2, NSE, and PBIAS between observed and simulated values.\n\n    Parameters:\n    -----------\n    obs : array-like\n        Observed values\n    sim : array-like\n        Simulated/predicted values\n\n    Returns:\n    --------\n    dict : Dictionary containing R\u00b2, NSE, and PBIAS\n\n    Example:\n    --------\n    &gt;&gt;&gt; obs = np.array([1.2, 2.3, 3.1, 4.5, 5.2])\n    &gt;&gt;&gt; sim = np.array([1.3, 2.1, 3.3, 4.2, 5.5])\n    &gt;&gt;&gt; results = evaluate_model(obs, sim)\n    &gt;&gt;&gt; print(f\"R\u00b2 = {results['R\u00b2']:.3f}\")\n    &gt;&gt;&gt; print(f\"NSE = {results['NSE']:.3f}\")\n    &gt;&gt;&gt; print(f\"PBIAS = {results['PBIAS']:.2f}%\")\n    \"\"\"\n    # Convert to numpy arrays\n    obs = np.array(obs)\n    sim = np.array(sim)\n\n    # R\u00b2 (Coefficient of Determination)\n    r = np.corrcoef(obs, sim)[0, 1]\n    r2 = r ** 2\n\n    # NSE (Nash-Sutcliffe Efficiency)\n    nse = 1 - np.sum((obs - sim) ** 2) / np.sum((obs - np.mean(obs)) ** 2)\n\n    # PBIAS (Percent Bias)\n    pbias = 100 * np.sum(obs - sim) / np.sum(obs)\n\n    return {'R\u00b2': r2, 'NSE': nse, 'PBIAS': pbias}\n</code></pre>"},{"location":"fundamentals/performance-metrics/#additional-metrics","title":"\ud83d\udcca Additional Metrics","text":""},{"location":"fundamentals/performance-metrics/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"\\[ \\mathrm{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |Q_{i}^{\\mathrm{obs}} - Q_{i}^{\\mathrm{sim}}| \\] <pre><code>from sklearn.metrics import mean_absolute_error\n\nmae = mean_absolute_error(observed, simulated)\n</code></pre>"},{"location":"fundamentals/performance-metrics/#root-mean-square-error-rmse","title":"Root Mean Square Error (RMSE)","text":"\\[ \\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (Q_{i}^{\\mathrm{obs}} - Q_{i}^{\\mathrm{sim}})^2} \\] <pre><code>from sklearn.metrics import mean_squared_error\n\nrmse = np.sqrt(mean_squared_error(observed, simulated))\n</code></pre>"},{"location":"fundamentals/performance-metrics/#visualization-of-performance","title":"\ud83d\udcc8 Visualization of Performance","text":""},{"location":"fundamentals/performance-metrics/#scatter-plot-with-metrics","title":"Scatter Plot with Metrics","text":"<pre><code>import matplotlib.pyplot as plt\n\ndef plot_performance(obs, sim, metrics):\n    \"\"\"\n    Create a scatter plot with performance metrics\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(8, 8))\n\n    # Scatter plot\n    ax.scatter(obs, sim, alpha=0.5, s=20)\n\n    # 1:1 line\n    min_val = min(obs.min(), sim.min())\n    max_val = max(obs.max(), sim.max())\n    ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='1:1 Line')\n\n    # Add metrics as text\n    textstr = f\"R\u00b2 = {metrics['R\u00b2']:.3f}\\n\"\n    textstr += f\"NSE = {metrics['NSE']:.3f}\\n\"\n    textstr += f\"PBIAS = {metrics['PBIAS']:.2f}%\"\n\n    ax.text(0.05, 0.95, textstr, transform=ax.transAxes,\n            fontsize=12, verticalalignment='top',\n            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n    ax.set_xlabel('Observed')\n    ax.set_ylabel('Simulated')\n    ax.set_title('Model Performance')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"fundamentals/performance-metrics/#performance-standards-for-hydrological-models","title":"\ud83c\udfaf Performance Standards for Hydrological Models","text":""},{"location":"fundamentals/performance-metrics/#monthly-time-step","title":"Monthly Time Step","text":"Performance Rating NSE R\u00b2 PBIAS Very Good &gt; 0.75 &gt; 0.75 &lt; \u00b110% Good 0.65-0.75 0.65-0.75 \u00b110-15% Satisfactory 0.50-0.65 0.50-0.65 \u00b115-25% Unsatisfactory &lt; 0.50 &lt; 0.50 &gt; \u00b125%"},{"location":"fundamentals/performance-metrics/#daily-time-step","title":"Daily Time Step","text":"Performance Rating NSE R\u00b2 PBIAS Very Good &gt; 0.65 &gt; 0.70 &lt; \u00b115% Good 0.50-0.65 0.60-0.70 \u00b115-20% Satisfactory 0.40-0.50 0.50-0.60 \u00b120-30% Unsatisfactory &lt; 0.40 &lt; 0.50 &gt; \u00b130% <p>Time Scale Matters</p> <p>Daily models typically have lower performance metrics than monthly models due to higher temporal variability.</p>"},{"location":"fundamentals/performance-metrics/#best-practices","title":"\u2705 Best Practices","text":"<ol> <li>Use Multiple Metrics: No single metric tells the complete story</li> <li>Consider Time Scale: Adjust expectations based on temporal resolution</li> <li>Check Residuals: Look for patterns in prediction errors</li> <li>Validate on Independent Data: Always test on unseen data</li> <li>Report Uncertainty: Include confidence intervals when possible</li> </ol>"},{"location":"fundamentals/performance-metrics/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>Now that you understand performance metrics: - Apply them to evaluate your Simple Linear Regression model - Use them to compare different models - Learn about Feature Engineering to improve performance</p> <p>:material-arrow-left: Data Import</p> <p>:material-arrow-right: Feature Engineering</p>"},{"location":"models/ann/","title":"Artificial Neural Networks (ANN)","text":""},{"location":"models/ann/#overview","title":"\ud83e\udde0 Overview","text":"<p>Artificial Neural Networks are powerful machine learning models inspired by biological neural networks. They can capture complex, non-linear relationships in hydrological data that traditional regression methods might miss.</p>"},{"location":"models/ann/#network-architecture","title":"\ud83c\udfd7\ufe0f Network Architecture","text":""},{"location":"models/ann/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>The basic neuron computation: \\(\\(y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)\\)\\)</p> <p>Where: - \\(x_i\\) = Input features - \\(w_i\\) = Weights - \\(b\\) = Bias - \\(f\\) = Activation function</p>"},{"location":"models/ann/#implementation","title":"\ud83d\udd28 Implementation","text":""},{"location":"models/ann/#step-1-import-libraries","title":"Step 1: Import Libraries","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\ntf.random.set_seed(42)\n</code></pre>"},{"location":"models/ann/#step-2-data-preparation","title":"Step 2: Data Preparation","text":"<pre><code># Load data\ndf = pd.read_csv('Discharge_30years.csv', \n                 parse_dates=['Date'], \n                 index_col='Date')\n\n# Feature engineering (same as MLR)\nINPUTS = df.iloc[:, :-1]\nOUTPUT = df.iloc[:, -1]\n\n# Create lag features\nrain_lags = [1, 2, 3]\ntmax_lags = [1, 2, 3]\ntmin_lags = [1, 2, 3]\n\nfor lag in rain_lags:\n    INPUTS[f'Rainfall_lag{lag}'] = INPUTS['Rainfall'].shift(lag)\nfor lag in tmax_lags:\n    INPUTS[f'Tmax_lag{lag}'] = INPUTS['Tmax'].shift(lag)\nfor lag in tmin_lags:\n    INPUTS[f'Tmin_lag{lag}'] = INPUTS['Tmin'].shift(lag)\n\n# Clean data\nFEATURES = INPUTS.dropna()\nTARGET = OUTPUT.loc[FEATURES.index]\n</code></pre>"},{"location":"models/ann/#step-3-feature-scaling","title":"Step 3: Feature Scaling","text":"<pre><code># Scale features\nX_scaler = StandardScaler()\nX_scaled = X_scaler.fit_transform(FEATURES)\n\n# Scale target\ny_scaler = StandardScaler()\ny_scaled = y_scaler.fit_transform(TARGET.values.reshape(-1, 1)).flatten()\n\nprint(\"Scaling complete!\")\nprint(f\"Features mean: {X_scaled.mean():.4f}, std: {X_scaled.std():.4f}\")\nprint(f\"Target mean: {y_scaled.mean():.4f}, std: {y_scaled.std():.4f}\")\n</code></pre>"},{"location":"models/ann/#step-4-train-test-split","title":"Step 4: Train-Test Split","text":"<pre><code>X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n    X_scaled, y_scaled, \n    test_size=0.2, \n    shuffle=False\n)\n\nprint(f\"Training samples: {len(X_train_scaled)}\")\nprint(f\"Testing samples: {len(X_test_scaled)}\")\n</code></pre>"},{"location":"models/ann/#step-5-define-hyperparameters","title":"Step 5: Define Hyperparameters","text":"<pre><code># Model architecture\nINPUT_DIM = X_train_scaled.shape[1]\nHIDDEN_LAYERS = [32, 16]\nDROPOUTS = [0.3, 0.2]\nACTIVATION = 'relu'\n\n# Training parameters\nOPTIMIZER = 'adam'\nLEARNING_RATE = 0.001\nEPOCHS = 100\nBATCH_SIZE = 32\n\nprint(f\"Input dimension: {INPUT_DIM}\")\nprint(f\"Hidden layers: {HIDDEN_LAYERS}\")\nprint(f\"Activation: {ACTIVATION}\")\n</code></pre>"},{"location":"models/ann/#step-6-build-the-model","title":"Step 6: Build the Model","text":"<pre><code># Clear any existing models\nK.clear_session()\n\n# Build model\nmodel = Sequential()\n\n# First hidden layer with input\nmodel.add(Dense(HIDDEN_LAYERS[0], \n                input_dim=INPUT_DIM, \n                activation=ACTIVATION))\nmodel.add(Dropout(DROPOUTS[0]))\n\n# Additional hidden layers\nfor i, units in enumerate(HIDDEN_LAYERS[1:], 1):\n    model.add(Dense(units, activation=ACTIVATION))\n    if i &lt; len(DROPOUTS):\n        model.add(Dropout(DROPOUTS[i]))\n\n# Output layer\nmodel.add(Dense(1))  # No activation for regression\n\n# Compile model\noptimizer = Adam(learning_rate=LEARNING_RATE)\nmodel.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n\n# Model summary\nmodel.summary()\n</code></pre>"},{"location":"models/ann/#step-7-train-the-model","title":"Step 7: Train the Model","text":"<pre><code># Train with early stopping\nfrom keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(\n    monitor='loss',\n    patience=10,\n    restore_best_weights=True\n)\n\nhistory = model.fit(\n    X_train_scaled, y_train_scaled,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    validation_split=0.1,\n    callbacks=[early_stop],\n    verbose=1\n)\n</code></pre>"},{"location":"models/ann/#step-8-make-predictions","title":"Step 8: Make Predictions","text":"<pre><code># Predict on test set\ny_pred_scaled = model.predict(X_test_scaled)\n\n# Inverse transform to original scale\ny_pred = y_scaler.inverse_transform(y_pred_scaled)\ny_true = y_scaler.inverse_transform(y_test_scaled.reshape(-1, 1))\n\nprint(f\"Predictions shape: {y_pred.shape}\")\n</code></pre>"},{"location":"models/ann/#model-evaluation","title":"\ud83d\udcc8 Model Evaluation","text":"<pre><code># Evaluate performance\ndef evaluate_model(obs, sim):\n    obs = np.array(obs).flatten()\n    sim = np.array(sim).flatten()\n\n    r = np.corrcoef(obs, sim)[0, 1]\n    r2 = r ** 2\n    nse = 1 - np.sum((obs - sim) ** 2) / np.sum((obs - np.mean(obs)) ** 2)\n    pbias = 100 * np.sum(obs - sim) / np.sum(obs)\n\n    return {'R\u00b2': r2, 'NSE': nse, 'PBIAS': pbias}\n\nresults = evaluate_model(y_true, y_pred)\nprint(f\"Test Performance:\")\nprint(f\"  R\u00b2 = {results['R\u00b2']:.4f}\")\nprint(f\"  NSE = {results['NSE']:.4f}\")\nprint(f\"  PBIAS = {results['PBIAS']:.2f}%\")\n</code></pre>"},{"location":"models/ann/#visualizations","title":"\ud83d\udcca Visualizations","text":""},{"location":"models/ann/#training-history","title":"Training History","text":"<pre><code>fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Loss\naxes[0].plot(history.history['loss'], label='Training Loss')\nif 'val_loss' in history.history:\n    axes[0].plot(history.history['val_loss'], label='Validation Loss')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss (MSE)')\naxes[0].set_title('Model Loss During Training')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# MAE\naxes[1].plot(history.history['mae'], label='Training MAE')\nif 'val_mae' in history.history:\n    axes[1].plot(history.history['val_mae'], label='Validation MAE')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('MAE')\naxes[1].set_title('Mean Absolute Error During Training')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/ann/#predictions-vs-observations","title":"Predictions vs Observations","text":"<pre><code>plt.figure(figsize=(14, 6))\n\n# Time series plot\ntest_dates = TARGET.iloc[-len(y_true):].index\nplt.plot(test_dates, y_true, 'b-', label='Observed', alpha=0.7)\nplt.plot(test_dates, y_pred, 'r-', label='ANN Predicted', alpha=0.7)\n\nplt.xlabel('Date')\nplt.ylabel('Discharge (m\u00b3/s)')\nplt.title('ANN: Observed vs Predicted Discharge')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/ann/#hyperparameter-tuning","title":"\ud83c\udf9b\ufe0f Hyperparameter Tuning","text":""},{"location":"models/ann/#grid-search-example","title":"Grid Search Example","text":"<pre><code>def create_model(hidden_layers, activation, learning_rate):\n    K.clear_session()\n    model = Sequential()\n\n    # First layer\n    model.add(Dense(hidden_layers[0], \n                   input_dim=INPUT_DIM, \n                   activation=activation))\n    model.add(Dropout(0.2))\n\n    # Additional layers\n    for units in hidden_layers[1:]:\n        model.add(Dense(units, activation=activation))\n        model.add(Dropout(0.2))\n\n    # Output\n    model.add(Dense(1))\n\n    # Compile\n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mse')\n\n    return model\n\n# Define hyperparameter grid\nparam_grid = {\n    'hidden_layers': [[32, 16], [64, 32], [128, 64, 32]],\n    'activation': ['relu', 'tanh'],\n    'learning_rate': [0.001, 0.01, 0.0001]\n}\n\n# Note: Full grid search implementation would go here\n</code></pre>"},{"location":"models/ann/#activation-functions","title":"\ud83c\udfaf Activation Functions","text":""},{"location":"models/ann/#common-activation-functions","title":"Common Activation Functions","text":"Function Formula Range Use Case ReLU \\(f(x) = max(0, x)\\) [0, \u221e) Hidden layers (default) Tanh \\(f(x) = tanh(x)\\) [-1, 1] Hidden layers (centered) Sigmoid \\(f(x) = 1/(1 + e^{-x})\\) [0, 1] Binary classification Linear \\(f(x) = x\\) (-\u221e, \u221e) Regression output"},{"location":"models/ann/#learning-rate-impact","title":"\ud83d\udcda Learning Rate Impact","text":""},{"location":"models/ann/#best-practices","title":"\u2705 Best Practices","text":"<ol> <li>Always scale inputs: Neural networks are sensitive to input scale</li> <li>Start simple: Begin with fewer layers and neurons</li> <li>Monitor overfitting: Use validation data and early stopping</li> <li>Experiment with architectures: Try different layer configurations</li> <li>Use dropout: Prevents overfitting by randomly dropping neurons</li> <li>Set random seeds: For reproducible results</li> <li>Save best models: Use callbacks to save the best performing model</li> </ol>"},{"location":"models/ann/#common-issues-and-solutions","title":"\u26a0\ufe0f Common Issues and Solutions","text":"Issue Symptom Solution Overfitting Train loss &lt;&lt; Test loss Add dropout, reduce layers, early stopping Underfitting Poor performance overall Add layers/neurons, train longer Exploding gradients Loss becomes NaN Reduce learning rate, use gradient clipping Slow convergence Loss decreases slowly Increase learning rate, change optimizer Unstable training Loss fluctuates wildly Reduce learning rate, increase batch size"},{"location":"models/ann/#model-comparison","title":"\ud83d\udd04 Model Comparison","text":"<pre><code># Compare all three models\ncomparison = pd.DataFrame({\n    'Model': ['SLR', 'MLR', 'ANN'],\n    'R\u00b2': [0.65, 0.78, results['R\u00b2']],\n    'NSE': [0.60, 0.75, results['NSE']],\n    'PBIAS': [5.2, 2.1, results['PBIAS']],\n    'Training Time': ['&lt;1s', '&lt;1s', '~30s'],\n    'Interpretability': ['High', 'High', 'Low'],\n    'Complexity': ['Low', 'Medium', 'High']\n})\n\nprint(\"Model Comparison:\")\nprint(comparison.to_string(index=False))\n</code></pre>"},{"location":"models/ann/#advanced-techniques","title":"\ud83d\ude80 Advanced Techniques","text":""},{"location":"models/ann/#1-lstm-for-time-series","title":"1. LSTM for Time Series","text":"<pre><code>from keras.layers import LSTM\n\n# LSTM model for sequential data\nlstm_model = Sequential([\n    LSTM(50, activation='relu', input_shape=(n_steps, n_features)),\n    Dropout(0.2),\n    Dense(25, activation='relu'),\n    Dense(1)\n])\n</code></pre>"},{"location":"models/ann/#2-ensemble-methods","title":"2. Ensemble Methods","text":"<pre><code># Combine predictions from multiple models\nensemble_pred = (mlr_pred + ann_pred) / 2\n</code></pre>"},{"location":"models/ann/#3-hyperparameter-optimization-with-keras-tuner","title":"3. Hyperparameter Optimization with Keras Tuner","text":"<pre><code># pip install keras-tuner\nimport keras_tuner as kt\n\ndef build_model(hp):\n    model = Sequential()\n\n    # Tune the number of layers and neurons\n    for i in range(hp.Int('n_layers', 2, 4)):\n        model.add(Dense(\n            hp.Int(f'units_{i}', 16, 128, step=16),\n            activation='relu'\n        ))\n        model.add(Dropout(hp.Float(f'dropout_{i}', 0, 0.5, step=0.1)))\n\n    model.add(Dense(1))\n\n    model.compile(\n        optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n        loss='mse'\n    )\n\n    return model\n</code></pre>"},{"location":"models/ann/#final-comparison-visualization","title":"\ud83d\udcca Final Comparison Visualization","text":"<pre><code># Create comprehensive comparison plot\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\nmodels = {\n    'SLR': {'pred': slr_pred, 'color': 'green'},\n    'MLR': {'pred': mlr_pred, 'color': 'blue'},\n    'ANN': {'pred': y_pred.flatten(), 'color': 'red'}\n}\n\n# 1. All models time series\nax = axes[0, 0]\nax.plot(test_dates, y_true, 'k-', label='Observed', linewidth=2, alpha=0.7)\nfor name, data in models.items():\n    ax.plot(test_dates[:len(data['pred'])], data['pred'], \n           color=data['color'], label=name, alpha=0.7)\nax.set_xlabel('Date')\nax.set_ylabel('Discharge (m\u00b3/s)')\nax.set_title('All Models Comparison')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# 2. Scatter plots for each model\nfor idx, (name, data) in enumerate(models.items()):\n    row = (idx + 1) // 2\n    col = (idx + 1) % 2\n    ax = axes[row, col]\n\n    ax.scatter(y_true[:len(data['pred'])], data['pred'], \n              alpha=0.5, s=20, color=data['color'])\n\n    # Perfect prediction line\n    min_val = min(y_true.min(), data['pred'].min())\n    max_val = max(y_true.max(), data['pred'].max())\n    ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)\n\n    ax.set_xlabel('Observed')\n    ax.set_ylabel('Predicted')\n    ax.set_title(f'{name} Model')\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Comprehensive Model Comparison', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/ann/#when-to-use-anns","title":"\ud83c\udfaf When to Use ANNs","text":""},{"location":"models/ann/#use-anns-when","title":"\u2705 Use ANNs when:","text":"<ul> <li>You have large datasets (&gt;5000 samples)</li> <li>Relationships are highly non-linear</li> <li>Multiple complex interactions exist</li> <li>High accuracy is more important than interpretability</li> <li>You have computational resources</li> </ul>"},{"location":"models/ann/#avoid-anns-when","title":"\u274c Avoid ANNs when:","text":"<ul> <li>Dataset is small (&lt;1000 samples)</li> <li>Interpretability is crucial</li> <li>Quick results are needed</li> <li>Limited computational resources</li> <li>Simple relationships exist</li> </ul>"},{"location":"models/ann/#assignment","title":"\ud83d\udcda Assignment","text":"<p>As mentioned in the original notebook, evaluate and visualize model performance over training data as well as test data to check for overfitting:</p> <pre><code># Predict on training data\ny_train_pred_scaled = model.predict(X_train_scaled)\ny_train_pred = y_scaler.inverse_transform(y_train_pred_scaled)\ny_train_true = y_scaler.inverse_transform(y_train_scaled.reshape(-1, 1))\n\n# Evaluate training performance\ntrain_results = evaluate_model(y_train_true, y_train_pred)\ntest_results = evaluate_model(y_true, y_pred)\n\n# Compare\nprint(\"Performance Comparison:\")\nprint(f\"Training R\u00b2: {train_results['R\u00b2']:.4f} | Test R\u00b2: {test_results['R\u00b2']:.4f}\")\nprint(f\"Training NSE: {train_results['NSE']:.4f} | Test NSE: {test_results['NSE']:.4f}\")\n\n# Check for overfitting\nif train_results['R\u00b2'] - test_results['R\u00b2'] &gt; 0.1:\n    print(\"\u26a0\ufe0f Warning: Model may be overfitting!\")\n</code></pre>"},{"location":"models/ann/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ol> <li>Experiment with different architectures</li> <li>Try LSTM or GRU for better time series modeling</li> <li>Implement cross-validation for robust evaluation</li> <li>Explore Time Series Libraries for advanced models</li> <li>Apply transfer learning from pre-trained models</li> </ol>"},{"location":"models/ann/#further-reading","title":"\ud83d\udcd6 Further Reading","text":"<ul> <li>Deep Learning for Time Series Forecasting</li> <li>TensorFlow Time Series Tutorial</li> <li>Keras Documentation</li> </ul> <p>:material-arrow-left: Multiple Linear Regression</p> <p>:material-arrow-right: Time Series Libraries</p>"},{"location":"models/mlr/","title":"Multiple Linear Regression (MLR)","text":""},{"location":"models/mlr/#overview","title":"\ud83d\udcca Overview","text":"<p>Multiple Linear Regression extends Simple Linear Regression by incorporating multiple independent variables to predict the dependent variable. In hydrology, this allows us to consider rainfall, temperature, and other factors simultaneously.</p>"},{"location":"models/mlr/#mathematical-foundation","title":"\ud83c\udfaf Mathematical Foundation","text":""},{"location":"models/mlr/#the-mlr-equation","title":"The MLR Equation","text":"\\[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\varepsilon \\] <p>Where: - \\(y\\) = Dependent variable (Discharge) - \\(x_1, x_2, ..., x_n\\) = Independent variables (Rainfall, Tmax, Tmin, lag features) - \\(\\beta_0\\) = Intercept - \\(\\beta_1, \\beta_2, ..., \\beta_n\\) = Coefficients for each variable - \\(\\varepsilon\\) = Error term</p>"},{"location":"models/mlr/#visual-representation","title":"Visual Representation","text":"<pre><code>graph LR\n    A[Rainfall] --&gt; E[MLR Model]\n    B[Tmax] --&gt; E\n    C[Tmin] --&gt; E\n    D[Lag Features] --&gt; E\n    E --&gt; F[Discharge Prediction]\n    G[Error \u03b5] --&gt; E</code></pre>"},{"location":"models/mlr/#implementation","title":"\ud83d\udd28 Implementation","text":""},{"location":"models/mlr/#step-1-import-libraries-and-load-data","title":"Step 1: Import Libraries and Load Data","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data\ndf = pd.read_csv('Discharge_30years.csv', \n                 parse_dates=['Date'], \n                 index_col='Date')\ndf = df.sort_values('Date')\n</code></pre>"},{"location":"models/mlr/#step-2-feature-engineering","title":"Step 2: Feature Engineering","text":"<pre><code># Separate inputs and outputs\nINPUTS = df.iloc[:, :-1]  # All columns except Discharge\nOUTPUT = df.iloc[:, -1]    # Discharge column\n\n# Create lag features based on CCF analysis\nrain_lags = [1, 2, 3]\ntmax_lags = [1, 2, 3]\ntmin_lags = [1, 2, 3]\n\n# Add lag features\nfor lag in rain_lags:\n    INPUTS[f'Rainfall_lag{lag}'] = INPUTS['Rainfall'].shift(lag)\n\nfor lag in tmax_lags:\n    INPUTS[f'Tmax_lag{lag}'] = INPUTS['Tmax'].shift(lag)\n\nfor lag in tmin_lags:\n    INPUTS[f'Tmin_lag{lag}'] = INPUTS['Tmin'].shift(lag)\n\n# Remove rows with NaN values\nFEATURES = INPUTS.dropna()\nTARGET = OUTPUT.loc[FEATURES.index]\n\nprint(f\"Features shape: {FEATURES.shape}\")\nprint(f\"Target shape: {TARGET.shape}\")\nprint(f\"\\nFeature columns:\")\nprint(FEATURES.columns.tolist())\n</code></pre>"},{"location":"models/mlr/#step-3-train-test-split","title":"Step 3: Train-Test Split","text":"<pre><code># Split data: 80% training, 20% testing\nX_mlr_train, X_mlr_test, y_mlr_train, y_mlr_test = train_test_split(\n    FEATURES, TARGET, \n    test_size=0.2, \n    shuffle=False  # Maintain temporal order\n)\n\nprint(f\"Training samples: {len(X_mlr_train)}\")\nprint(f\"Testing samples: {len(X_mlr_test)}\")\n</code></pre>"},{"location":"models/mlr/#step-4-build-and-train-mlr-model","title":"Step 4: Build and Train MLR Model","text":"<pre><code># Initialize and train the model\nmlr_model = LinearRegression()\nmlr_model.fit(X_mlr_train, y_mlr_train)\n\n# Make predictions\ny_mlr_pred = mlr_model.predict(X_mlr_test)\n\nprint(\"Model trained successfully!\")\n</code></pre>"},{"location":"models/mlr/#step-5-extract-model-equation","title":"Step 5: Extract Model Equation","text":"<pre><code># Get coefficients and intercept\nfeature_names = FEATURES.columns\ncoefficients = mlr_model.coef_\nintercept = mlr_model.intercept_\n\n# Create equation string\nequation = f\"Discharge = {intercept:.3f}\"\nfor name, coef in zip(feature_names, coefficients):\n    if coef &gt;= 0:\n        equation += f\" + {coef:.4f}\u00d7{name}\"\n    else:\n        equation += f\" - {abs(coef):.4f}\u00d7{name}\"\n\nprint(\"Fitted Equation:\")\nprint(\"=\"*50)\nprint(equation)\nprint(\"=\"*50)\n\n# Create a DataFrame for better visualization\ncoef_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Coefficient': coefficients\n}).sort_values('Coefficient', ascending=False)\n\nprint(\"\\nFeature Coefficients (sorted by magnitude):\")\nprint(coef_df)\n</code></pre>"},{"location":"models/mlr/#model-evaluation","title":"\ud83d\udcc8 Model Evaluation","text":""},{"location":"models/mlr/#performance-metrics","title":"Performance Metrics","text":"<pre><code>def evaluate_model(obs, sim):\n    \"\"\"Calculate R\u00b2, NSE, and PBIAS\"\"\"\n    obs = np.array(obs)\n    sim = np.array(sim)\n\n    # R\u00b2\n    r = np.corrcoef(obs, sim)[0, 1]\n    r2 = r ** 2\n\n    # NSE\n    nse = 1 - np.sum((obs - sim) ** 2) / np.sum((obs - np.mean(obs)) ** 2)\n\n    # PBIAS\n    pbias = 100 * np.sum(obs - sim) / np.sum(obs)\n\n    return {'R\u00b2': r2, 'NSE': nse, 'PBIAS': pbias}\n\n# Evaluate the model\nresults = evaluate_model(y_mlr_test, y_mlr_pred)\n\nprint(\"Model Performance:\")\nprint(f\"  R\u00b2 = {results['R\u00b2']:.4f}\")\nprint(f\"  NSE = {results['NSE']:.4f}\")\nprint(f\"  PBIAS = {results['PBIAS']:.2f}%\")\n</code></pre>"},{"location":"models/mlr/#performance-comparison-table","title":"Performance Comparison Table","text":"<pre><code># Compare with SLR (if you've run it)\ncomparison_df = pd.DataFrame({\n    'Model': ['SLR (Rainfall only)', 'MLR (All features)'],\n    'R\u00b2': [0.65, results['R\u00b2']],  # Example SLR value\n    'NSE': [0.60, results['NSE']],\n    'PBIAS': [5.2, results['PBIAS']]\n})\n\nprint(\"\\nModel Comparison:\")\nprint(comparison_df)\n</code></pre>"},{"location":"models/mlr/#visualizations","title":"\ud83d\udcca Visualizations","text":""},{"location":"models/mlr/#1-predicted-vs-observed-scatter-plot","title":"1. Predicted vs Observed Scatter Plot","text":"<pre><code>plt.figure(figsize=(10, 8))\n\n# Scatter plot\nplt.scatter(y_mlr_test, y_mlr_pred, alpha=0.5, s=20, label='Predictions')\n\n# Perfect prediction line\nmin_val = min(y_mlr_test.min(), y_mlr_pred.min())\nmax_val = max(y_mlr_test.max(), y_mlr_pred.max())\nplt.plot([min_val, max_val], [min_val, max_val], 'r--', \n         label='Perfect Prediction', linewidth=2)\n\n# Add metrics to plot\ntextstr = f'R\u00b2 = {results[\"R\u00b2\"]:.3f}\\nNSE = {results[\"NSE\"]:.3f}\\nPBIAS = {results[\"PBIAS\"]:.2f}%'\nplt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes,\n         fontsize=12, verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n\nplt.xlabel('Observed Discharge (m\u00b3/s)', fontsize=12)\nplt.ylabel('Predicted Discharge (m\u00b3/s)', fontsize=12)\nplt.title('MLR: Predicted vs Observed Discharge', fontsize=14, fontweight='bold')\nplt.legend(loc='lower right')\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/mlr/#2-time-series-comparison","title":"2. Time Series Comparison","text":"<pre><code>plt.figure(figsize=(15, 6))\n\n# Get test dates\ntest_dates = y_mlr_test.index\n\n# Plot observed and predicted\nplt.plot(test_dates, y_mlr_test.values, 'b-', label='Observed', \n         linewidth=1.5, alpha=0.7)\nplt.plot(test_dates, y_mlr_pred, 'r-', label='Predicted', \n         linewidth=1.5, alpha=0.7)\n\n# Fill between for error visualization\nplt.fill_between(test_dates, y_mlr_test.values, y_mlr_pred, \n                 alpha=0.2, color='gray')\n\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('Discharge (m\u00b3/s)', fontsize=12)\nplt.title('MLR: Time Series Comparison', fontsize=14, fontweight='bold')\nplt.legend(loc='upper right')\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/mlr/#3-feature-importance-visualization","title":"3. Feature Importance Visualization","text":"<pre><code># Create feature importance plot\nplt.figure(figsize=(10, 8))\n\n# Sort coefficients by absolute value\ncoef_sorted = coef_df.iloc[np.argsort(np.abs(coef_df['Coefficient'].values))[::-1]]\n\n# Create horizontal bar plot\ncolors = ['green' if x &gt; 0 else 'red' for x in coef_sorted['Coefficient']]\nplt.barh(range(len(coef_sorted)), coef_sorted['Coefficient'], color=colors)\nplt.yticks(range(len(coef_sorted)), coef_sorted['Feature'])\nplt.xlabel('Coefficient Value', fontsize=12)\nplt.title('Feature Importance in MLR Model', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3, axis='x')\n\n# Add vertical line at x=0\nplt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/mlr/#4-residual-analysis","title":"4. Residual Analysis","text":"<pre><code>fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Calculate residuals\nresiduals = y_mlr_test - y_mlr_pred\n\n# 1. Residuals vs Fitted\naxes[0, 0].scatter(y_mlr_pred, residuals, alpha=0.5, s=20)\naxes[0, 0].axhline(y=0, color='r', linestyle='--')\naxes[0, 0].set_xlabel('Fitted Values')\naxes[0, 0].set_ylabel('Residuals')\naxes[0, 0].set_title('Residuals vs Fitted Values')\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Q-Q Plot\nfrom scipy import stats\nstats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\naxes[0, 1].set_title('Q-Q Plot')\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Histogram of Residuals\naxes[1, 0].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\naxes[1, 0].set_xlabel('Residuals')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].set_title('Distribution of Residuals')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Add normal distribution overlay\nmu, std = residuals.mean(), residuals.std()\nx = np.linspace(residuals.min(), residuals.max(), 100)\naxes[1, 0].plot(x, stats.norm.pdf(x, mu, std) * len(residuals) * (residuals.max() - residuals.min()) / 30, \n                'r-', linewidth=2, label='Normal')\naxes[1, 0].legend()\n\n# 4. Residuals over Time\naxes[1, 1].plot(test_dates, residuals, 'o-', alpha=0.5, markersize=4)\naxes[1, 1].axhline(y=0, color='r', linestyle='--')\naxes[1, 1].set_xlabel('Date')\naxes[1, 1].set_ylabel('Residuals')\naxes[1, 1].set_title('Residuals over Time')\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].tick_params(axis='x', rotation=45)\n\nplt.suptitle('MLR Model Diagnostics', fontsize=16, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/mlr/#model-interpretation","title":"\ud83d\udd0d Model Interpretation","text":""},{"location":"models/mlr/#understanding-coefficients","title":"Understanding Coefficients","text":"<pre><code># Analyze coefficient signs and magnitudes\nprint(\"Model Interpretation:\")\nprint(\"=\"*50)\nprint(f\"Intercept: {intercept:.3f}\")\nprint(\"  \u2192 Base discharge when all features = 0\")\nprint()\n\nfor name, coef in zip(feature_names, coefficients):\n    if 'Rainfall' in name:\n        print(f\"{name}: {coef:.4f}\")\n        if coef &gt; 0:\n            print(f\"  \u2192 1 mm increase in {name} increases discharge by {coef:.4f} m\u00b3/s\")\n    elif 'Tmax' in name:\n        print(f\"{name}: {coef:.4f}\")\n        if coef &gt; 0:\n            print(f\"  \u2192 1\u00b0C increase in {name} increases discharge by {coef:.4f} m\u00b3/s\")\n        else:\n            print(f\"  \u2192 1\u00b0C increase in {name} decreases discharge by {abs(coef):.4f} m\u00b3/s\")\n</code></pre>"},{"location":"models/mlr/#advantages-and-limitations","title":"\ud83c\udfaf Advantages and Limitations","text":""},{"location":"models/mlr/#advantages-of-mlr","title":"\u2705 Advantages of MLR","text":"<ul> <li>Multiple Factors: Considers various influences simultaneously</li> <li>Interpretable: Clear understanding of each variable's contribution</li> <li>Fast: Quick to train and predict</li> <li>No Hyperparameters: Simple to implement</li> <li>Statistical Inference: Can provide confidence intervals</li> </ul>"},{"location":"models/mlr/#limitations","title":"\u274c Limitations","text":"<ul> <li>Linearity Assumption: Assumes linear relationships</li> <li>Multicollinearity: Correlated features can cause issues</li> <li>No Interaction Capture: Doesn't model feature interactions automatically</li> <li>Sensitive to Outliers: Can be skewed by extreme values</li> <li>Assumes Independence: Residuals should be independent</li> </ul>"},{"location":"models/mlr/#improvements-and-extensions","title":"\ud83d\ude80 Improvements and Extensions","text":""},{"location":"models/mlr/#1-feature-selection","title":"1. Feature Selection","text":"<pre><code>from sklearn.feature_selection import SelectKBest, f_regression\n\n# Select top K features\nselector = SelectKBest(score_func=f_regression, k=10)\nX_selected = selector.fit_transform(X_mlr_train, y_mlr_train)\n\n# Get selected feature names\nselected_features = FEATURES.columns[selector.get_support()].tolist()\nprint(f\"Selected features: {selected_features}\")\n</code></pre>"},{"location":"models/mlr/#2-regularization-ridgelasso","title":"2. Regularization (Ridge/Lasso)","text":"<pre><code>from sklearn.linear_model import Ridge, Lasso\n\n# Ridge Regression\nridge_model = Ridge(alpha=1.0)\nridge_model.fit(X_mlr_train, y_mlr_train)\n\n# Lasso Regression\nlasso_model = Lasso(alpha=0.1)\nlasso_model.fit(X_mlr_train, y_mlr_train)\n</code></pre>"},{"location":"models/mlr/#3-polynomial-features","title":"3. Polynomial Features","text":"<pre><code>from sklearn.preprocessing import PolynomialFeatures\n\n# Create polynomial features\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X_mlr_train)\n</code></pre>"},{"location":"models/mlr/#next-steps","title":"\ud83d\udcda Next Steps","text":"<p>Now that you've mastered MLR: 1. Try Artificial Neural Networks for non-linear patterns 2. Experiment with more lag features 3. Apply regularization techniques 4. Compare with advanced models</p> <p>:material-arrow-left: Simple Linear Regression</p> <p>:material-arrow-right: Artificial Neural Network</p>"},{"location":"models/slr/","title":"Simple Linear Regression (SLR)","text":""},{"location":"models/slr/#overview","title":"\ud83d\udcca Overview","text":"<p>Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (\\(y\\)) and a single independent variable (\\(x\\)). In hydrology, we'll use it to predict discharge based on rainfall.</p>"},{"location":"models/slr/#mathematical-foundation","title":"\ud83c\udfaf Mathematical Foundation","text":""},{"location":"models/slr/#the-slr-equation","title":"The SLR Equation","text":"\\[ y = \\beta_0 + \\beta_1 x + \\varepsilon \\] <p>Where: - \\(y\\) = Dependent variable (Discharge) - \\(x\\) = Independent variable (Rainfall) - \\(\\beta_0\\) = Intercept (discharge when rainfall = 0) - \\(\\beta_1\\) = Slope (change in discharge per unit rainfall) - \\(\\varepsilon\\) = Random error term</p>"},{"location":"models/slr/#visual-representation","title":"Visual Representation","text":"<pre><code>graph LR\n    A[Rainfall&lt;br/&gt;Input X] --&gt; B[Linear Model&lt;br/&gt;y = \u03b2\u2080 + \u03b2\u2081x]\n    B --&gt; C[Discharge&lt;br/&gt;Output Y]\n    D[Error \u03b5] --&gt; B</code></pre>"},{"location":"models/slr/#implementation-steps","title":"\ud83d\udd28 Implementation Steps","text":""},{"location":"models/slr/#step-1-data-preparation","title":"Step 1: Data Preparation","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_csv('Discharge_30years.csv', \n                 parse_dates=['Date'], \n                 index_col='Date')\ndf = df.sort_values('Date')\n</code></pre>"},{"location":"models/slr/#step-2-define-inputs-and-outputs","title":"Step 2: Define Inputs and Outputs","text":"<pre><code># For SLR, we use only Rainfall as input\nX_slr = df[['Rainfall']]  # DataFrame with shape (n, 1)\ny_slr = df['Discharge']    # Series with shape (n,)\n\nprint(f\"Input shape: {X_slr.shape}\")\nprint(f\"Output shape: {y_slr.shape}\")\n</code></pre> <p>Why Double Brackets?</p> <p>We use <code>df[['Rainfall']]</code> with double brackets to get a DataFrame (2D), not <code>df['Rainfall']</code> which returns a Series (1D). Scikit-learn expects 2D input.</p>"},{"location":"models/slr/#step-3-train-test-split","title":"Step 3: Train-Test Split","text":"<pre><code># Split data: 80% training, 20% testing\nX_slr_train, X_slr_test, y_slr_train, y_slr_test = train_test_split(\n    X_slr, y_slr, \n    test_size=0.2, \n    shuffle=False  # Important for time series!\n)\n\nprint(f\"Training samples: {len(X_slr_train)}\")\nprint(f\"Testing samples: {len(X_slr_test)}\")\n</code></pre> <p>Time Series Consideration</p> <p>We set <code>shuffle=False</code> to maintain temporal order. Shuffling time series data can lead to data leakage and unrealistic performance estimates.</p>"},{"location":"models/slr/#step-4-build-and-train-model","title":"Step 4: Build and Train Model","text":"<pre><code># Initialize the model\nslr_model = LinearRegression()\n\n# Train the model\nslr_model.fit(X_slr_train, y_slr_train)\n\n# Make predictions\ny_slr_pred = slr_model.predict(X_slr_test)\n</code></pre>"},{"location":"models/slr/#step-5-extract-model-parameters","title":"Step 5: Extract Model Parameters","text":"<pre><code># Get the fitted equation parameters\nslope = slr_model.coef_[0]\nintercept = slr_model.intercept_\n\nprint(f\"Fitted Equation: Discharge = {intercept:.4f} + {slope:.4f} \u00d7 Rainfall\")\nprint(f\"Interpretation:\")\nprint(f\"  - Base discharge (no rain): {intercept:.4f} m\u00b3/s\")\nprint(f\"  - Discharge increase per mm rain: {slope:.4f} m\u00b3/s\")\n</code></pre>"},{"location":"models/slr/#model-evaluation","title":"\ud83d\udcc8 Model Evaluation","text":""},{"location":"models/slr/#performance-metrics-function","title":"Performance Metrics Function","text":"<pre><code>def evaluate_model(obs, sim):\n    \"\"\"\n    Calculate R\u00b2, NSE, and PBIAS\n    \"\"\"\n    obs = np.array(obs)\n    sim = np.array(sim)\n\n    # R\u00b2 (Coefficient of Determination)\n    r = np.corrcoef(obs, sim)[0, 1]\n    r2 = r ** 2\n\n    # NSE (Nash-Sutcliffe Efficiency)\n    nse = 1 - np.sum((obs - sim) ** 2) / np.sum((obs - np.mean(obs)) ** 2)\n\n    # PBIAS (Percent Bias)\n    pbias = 100 * np.sum(obs - sim) / np.sum(obs)\n\n    return {'R\u00b2': r2, 'NSE': nse, 'PBIAS': pbias}\n\n# Evaluate the model\nresults = evaluate_model(y_slr_test, y_slr_pred)\nprint(f\"Model Performance:\")\nprint(f\"  R\u00b2 = {results['R\u00b2']:.3f}\")\nprint(f\"  NSE = {results['NSE']:.3f}\")\nprint(f\"  PBIAS = {results['PBIAS']:.2f}%\")\n</code></pre>"},{"location":"models/slr/#performance-interpretation","title":"Performance Interpretation","text":"Metric Range Good Performance Your Model R\u00b2 0 to 1 &gt; 0.6 Check results NSE -\u221e to 1 &gt; 0.5 Check results PBIAS -\u221e to +\u221e -10% to +10% Check results"},{"location":"models/slr/#visualization","title":"\ud83d\udcca Visualization","text":""},{"location":"models/slr/#1-scatter-plot-with-regression-line","title":"1. Scatter Plot with Regression Line","text":"<pre><code>plt.figure(figsize=(10, 6))\n\n# Training data\nplt.scatter(X_slr_train, y_slr_train, alpha=0.5, s=10, \n            label='Training Data', color='blue')\n\n# Test data\nplt.scatter(X_slr_test, y_slr_test, alpha=0.5, s=10, \n            label='Test Data', color='green')\n\n# Regression line\nx_line = np.linspace(X_slr.min(), X_slr.max(), 100)\ny_line = slr_model.predict(x_line)\nplt.plot(x_line, y_line, 'r-', linewidth=2, \n         label=f'y = {intercept:.3f} + {slope:.3f}x')\n\nplt.xlabel('Rainfall (mm)')\nplt.ylabel('Discharge (m\u00b3/s)')\nplt.title('Simple Linear Regression: Rainfall vs Discharge')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"models/slr/#2-predicted-vs-observed","title":"2. Predicted vs Observed","text":"<pre><code>plt.figure(figsize=(8, 8))\n\n# Perfect prediction line\nmin_val = min(y_slr_test.min(), y_slr_pred.min())\nmax_val = max(y_slr_test.max(), y_slr_pred.max())\nplt.plot([min_val, max_val], [min_val, max_val], 'r--', \n         label='Perfect Prediction', alpha=0.7)\n\n# Scatter plot\nplt.scatter(y_slr_test, y_slr_pred, alpha=0.6, s=20)\n\n# Add R\u00b2 to plot\nplt.text(0.05, 0.95, f'R\u00b2 = {results[\"R\u00b2\"]:.3f}', \n         transform=plt.gca().transAxes,\n         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.xlabel('Observed Discharge (m\u00b3/s)')\nplt.ylabel('Predicted Discharge (m\u00b3/s)')\nplt.title('Predicted vs Observed Discharge')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.show()\n</code></pre>"},{"location":"models/slr/#3-time-series-comparison","title":"3. Time Series Comparison","text":"<pre><code>plt.figure(figsize=(14, 6))\n\n# Get test dates\ntest_dates = y_slr_test.index\n\nplt.plot(test_dates, y_slr_test.values, 'b-', label='Observed', alpha=0.7)\nplt.plot(test_dates, y_slr_pred, 'r-', label='Predicted', alpha=0.7)\n\nplt.fill_between(test_dates, y_slr_test.values, y_slr_pred, \n                 alpha=0.3, color='gray', label='Error')\n\nplt.xlabel('Date')\nplt.ylabel('Discharge (m\u00b3/s)')\nplt.title('Time Series: Observed vs Predicted Discharge')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/slr/#4-residual-analysis","title":"4. Residual Analysis","text":"<pre><code>fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Calculate residuals\nresiduals = y_slr_test - y_slr_pred\n\n# 1. Residuals vs Fitted\naxes[0, 0].scatter(y_slr_pred, residuals, alpha=0.6)\naxes[0, 0].axhline(y=0, color='r', linestyle='--')\naxes[0, 0].set_xlabel('Fitted Values')\naxes[0, 0].set_ylabel('Residuals')\naxes[0, 0].set_title('Residuals vs Fitted')\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Q-Q Plot\nfrom scipy import stats\nstats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\naxes[0, 1].set_title('Q-Q Plot')\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Histogram of Residuals\naxes[1, 0].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\naxes[1, 0].set_xlabel('Residuals')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].set_title('Histogram of Residuals')\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. Residuals over Time\naxes[1, 1].plot(test_dates, residuals, 'o-', alpha=0.6)\naxes[1, 1].axhline(y=0, color='r', linestyle='--')\naxes[1, 1].set_xlabel('Date')\naxes[1, 1].set_ylabel('Residuals')\naxes[1, 1].set_title('Residuals over Time')\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/slr/#model-limitations","title":"\ud83c\udfaf Model Limitations","text":"<p>SLR Limitations</p> <ol> <li>Linearity Assumption: Assumes a straight-line relationship</li> <li>Single Variable: Only considers rainfall, ignoring other factors</li> <li>No Lag Effects: Doesn't account for delayed response</li> <li>No Seasonality: Ignores seasonal patterns</li> <li>Homoscedasticity: Assumes constant variance of errors</li> </ol>"},{"location":"models/slr/#when-to-use-slr","title":"\ud83d\udca1 When to Use SLR","text":""},{"location":"models/slr/#good-for","title":"\u2705 Good for:","text":"<ul> <li>Quick baseline models</li> <li>Understanding basic relationships</li> <li>When you have limited data</li> <li>Educational purposes</li> <li>Initial exploratory analysis</li> </ul>"},{"location":"models/slr/#not-ideal-for","title":"\u274c Not ideal for:","text":"<ul> <li>Complex hydrological systems</li> <li>Multiple influencing factors</li> <li>Non-linear relationships</li> <li>Capturing temporal dependencies</li> </ul>"},{"location":"models/slr/#improvements","title":"\ud83d\ude80 Improvements","text":"<p>To improve your SLR model, consider:</p> <ol> <li> <p>Log Transformation: If relationship is exponential    <pre><code>X_log = np.log1p(X_slr)  # log(1 + x) to handle zeros\n</code></pre></p> </li> <li> <p>Polynomial Features: For curved relationships    <pre><code>from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X_slr)\n</code></pre></p> </li> <li> <p>Remove Outliers: Improve model stability    <pre><code># Remove points beyond 3 standard deviations\nz_scores = np.abs(stats.zscore(y_slr))\nmask = z_scores &lt; 3\nX_clean = X_slr[mask]\ny_clean = y_slr[mask]\n</code></pre></p> </li> </ol>"},{"location":"models/slr/#next-steps","title":"\ud83d\udcda Next Steps","text":"<p>Now that you understand SLR, proceed to: - Multiple Linear Regression - Add more variables - Feature Engineering - Create lag features - Neural Networks - Capture complex patterns</p> <p>:material-arrow-left: Feature Engineering</p> <p>:material-arrow-right: Multiple Linear Regression</p>"},{"location":"resources/resources-libraries/","title":"Time Series Forecasting Libraries","text":""},{"location":"resources/resources-libraries/#overview","title":"\ud83d\udcda Overview","text":"<p>While this guide teaches you to build models from scratch, there are excellent pre-built libraries that can accelerate your time series forecasting projects. Here's a comprehensive guide to the best libraries available for hydrological and general time series forecasting.</p>"},{"location":"resources/resources-libraries/#top-time-series-libraries","title":"\ud83c\udf1f Top Time Series Libraries","text":""},{"location":"resources/resources-libraries/#1-prophet-by-metafacebook","title":"1. Prophet (by Meta/Facebook)","text":"<p>Prophet is designed for forecasting time series data with strong seasonal patterns and holiday effects.</p> <p>Best for: Business forecasting, datasets with missing values, strong seasonality</p> <pre><code># Installation\npip install prophet\n\n# Basic usage\nfrom prophet import Prophet\nimport pandas as pd\n\n# Prepare data\ndf = pd.DataFrame({\n    'ds': dates,  # Date column\n    'y': values   # Value column\n})\n\n# Create and fit model\nmodel = Prophet()\nmodel.fit(df)\n\n# Make predictions\nfuture = model.make_future_dataframe(periods=365)\nforecast = model.predict(future)\n</code></pre> <p>Key Features: - Automatic seasonality detection - Holiday effects modeling - Robust to missing data - Uncertainty intervals</p> <p>\ud83d\udcd6 Documentation | \ud83d\udd17 GitHub</p>"},{"location":"resources/resources-libraries/#2-statsmodels-arima-sarimax","title":"2. statsmodels - ARIMA &amp; SARIMAX","text":"<p>Classical statistical models for time series analysis.</p> <p>Best for: Traditional time series analysis, academic research, interpretable models</p> <pre><code># ARIMA Example\nfrom statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(data, order=(1, 1, 1))  # (p, d, q)\nmodel_fit = model.fit()\npredictions = model_fit.forecast(steps=10)\n\n# SARIMAX with exogenous variables\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\nmodel = SARIMAX(\n    discharge_data,\n    exog=rainfall_data,\n    order=(1, 1, 1),\n    seasonal_order=(1, 1, 1, 12)\n)\n</code></pre> <p>Key Features: - ARIMA, SARIMA, SARIMAX models - Statistical tests (ADF, KPSS) - ACF/PACF plots - Comprehensive diagnostics</p> <p>\ud83d\udcd6 Documentation</p>"},{"location":"resources/resources-libraries/#3-darts-modern-time-series-library","title":"3. Darts - Modern Time Series Library","text":"<p>A Python library for easy manipulation and forecasting of time series.</p> <p>Best for: Modern deep learning approaches, multivariate forecasting, probabilistic forecasting</p> <pre><code># Installation\npip install darts\n\n# Example with N-BEATS model\nfrom darts import TimeSeries\nfrom darts.models import NBEATSModel\n\n# Convert pandas to Darts TimeSeries\nseries = TimeSeries.from_dataframe(df, 'date', 'discharge')\n\n# Split data\ntrain, val = series.split_before(0.8)\n\n# Create and train model\nmodel = NBEATSModel(\n    input_chunk_length=30,\n    output_chunk_length=7,\n    n_epochs=100\n)\nmodel.fit(train)\n\n# Predict\nprediction = model.predict(n=30)\n</code></pre> <p>Key Features: - 30+ forecasting models (classical &amp; neural) - Unified API - Probabilistic forecasting - Multivariate support - Backtesting utilities</p> <p>\ud83d\udcd6 Documentation | \ud83d\udd17 GitHub</p>"},{"location":"resources/resources-libraries/#4-neuralprophet-neural-networks-meet-prophet","title":"4. NeuralProphet - Neural Networks meet Prophet","text":"<p>Combines Prophet's interpretability with deep learning power.</p> <p>Best for: Complex seasonality, automatic feature engineering, interpretable neural networks</p> <pre><code># Installation\npip install neuralprophet\n\nfrom neuralprophet import NeuralProphet\n\nmodel = NeuralProphet(\n    n_forecasts=30,\n    n_lags=30,\n    yearly_seasonality=True,\n    weekly_seasonality=False,\n    daily_seasonality=False,\n)\n\nmodel.fit(df, freq='D')\nfuture = model.make_future_dataframe(df, periods=365)\nforecast = model.predict(future)\n</code></pre> <p>\ud83d\udcd6 Documentation | \ud83d\udd17 GitHub</p>"},{"location":"resources/resources-libraries/#5-sktime-unified-time-series-framework","title":"5. sktime - Unified Time Series Framework","text":"<p>Scikit-learn compatible library for time series.</p> <p>Best for: Machine learning pipelines, ensemble methods, time series classification</p> <pre><code># Installation\npip install sktime\n\nfrom sktime.forecasting.arima import AutoARIMA\nfrom sktime.forecasting.compose import make_reduction\n\n# AutoARIMA\nforecaster = AutoARIMA(sp=12, suppress_warnings=True)\nforecaster.fit(y_train)\ny_pred = forecaster.predict(fh=[1, 2, 3, 4, 5])\n\n# Using any sklearn regressor for forecasting\nfrom sklearn.ensemble import RandomForestRegressor\nforecaster = make_reduction(RandomForestRegressor(), window_length=10)\n</code></pre> <p>\ud83d\udcd6 Documentation | \ud83d\udd17 GitHub</p>"},{"location":"resources/resources-libraries/#hydrology-specific-libraries","title":"\ud83c\udf0a Hydrology-Specific Libraries","text":""},{"location":"resources/resources-libraries/#1-hydroshare-python-client","title":"1. HydroShare Python Client","text":"<p>For accessing hydrological data: <pre><code>pip install hs_restclient\n</code></pre></p>"},{"location":"resources/resources-libraries/#2-pyeto-evapotranspiration-calculations","title":"2. PyETo - Evapotranspiration Calculations","text":"<pre><code>pip install pyeto\n</code></pre>"},{"location":"resources/resources-libraries/#3-pastas-time-series-analysis-for-hydrology","title":"3. Pastas - Time Series Analysis for Hydrology","text":"<p>Specifically designed for groundwater analysis: <pre><code>pip install pastas\n\nimport pastas as ps\nmodel = ps.Model(head_series)\nmodel.add_stressmodel(ps.StressModel(precipitation, ps.Gamma))\n</code></pre></p> <p>\ud83d\udd17 Pastas Documentation</p>"},{"location":"resources/resources-libraries/#advanced-deep-learning-libraries","title":"\ud83d\ude80 Advanced Deep Learning Libraries","text":""},{"location":"resources/resources-libraries/#tensorflowkeras-time-series","title":"TensorFlow/Keras Time Series","text":"<p>For custom LSTM/GRU models:</p> <pre><code>from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\nmodel = Sequential([\n    LSTM(50, activation='relu', input_shape=(n_steps, n_features)),\n    Dense(1)\n])\n</code></pre>"},{"location":"resources/resources-libraries/#pytorch-forecasting","title":"PyTorch Forecasting","text":"<p>Advanced neural network models:</p> <pre><code>pip install pytorch-forecasting\n\nfrom pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n</code></pre>"},{"location":"resources/resources-libraries/#comparison-table","title":"\ud83d\udcca Comparison Table","text":"Library Best For Learning Curve Performance Interpretability Prophet Quick forecasting Easy Good High statsmodels Classical methods Medium Good Very High Darts Modern approaches Medium Excellent Medium NeuralProphet Complex patterns Medium Very Good High sktime ML pipelines Easy Good High TensorFlow Custom models Hard Excellent Low"},{"location":"resources/resources-libraries/#choosing-the-right-library","title":"\ud83c\udfaf Choosing the Right Library","text":""},{"location":"resources/resources-libraries/#decision-tree","title":"Decision Tree","text":"<pre><code>graph TD\n    A[Start] --&gt; B{Dataset Size?}\n    B --&gt;|Small &lt; 1000| C[statsmodels/Prophet]\n    B --&gt;|Medium 1000-10000| D[Prophet/NeuralProphet]\n    B --&gt;|Large &gt; 10000| E[Darts/TensorFlow]\n\n    C --&gt; F{Need Interpretability?}\n    D --&gt; F\n    E --&gt; F\n\n    F --&gt;|Yes| G[Prophet/statsmodels]\n    F --&gt;|No| H[Neural Networks]\n\n    G --&gt; I[Final Choice]\n    H --&gt; I</code></pre>"},{"location":"resources/resources-libraries/#quick-start-templates","title":"\ud83d\udcbb Quick Start Templates","text":""},{"location":"resources/resources-libraries/#hydrological-forecasting-pipeline","title":"Hydrological Forecasting Pipeline","text":"<pre><code># Combined approach for discharge prediction\nimport pandas as pd\nfrom prophet import Prophet\nfrom darts import TimeSeries\nfrom darts.models import NBEATSModel\n\n# 1. Quick baseline with Prophet\ndef prophet_baseline(df):\n    model = Prophet(\n        changepoint_prior_scale=0.05,\n        seasonality_mode='multiplicative'\n    )\n    model.add_regressor('rainfall')\n    model.add_regressor('temperature')\n    model.fit(df)\n    return model\n\n# 2. Advanced model with Darts\ndef neural_forecast(series, covariates):\n    model = NBEATSModel(\n        input_chunk_length=30,\n        output_chunk_length=7,\n        n_epochs=100\n    )\n    model.fit(\n        series, \n        past_covariates=covariates,\n        verbose=False\n    )\n    return model\n\n# 3. Ensemble approach\ndef ensemble_forecast(df):\n    prophet_pred = prophet_baseline(df).predict()\n    neural_pred = neural_forecast(df).predict()\n    return (prophet_pred + neural_pred) / 2\n</code></pre>"},{"location":"resources/resources-libraries/#learning-resources","title":"\ud83d\udcd6 Learning Resources","text":""},{"location":"resources/resources-libraries/#online-courses","title":"Online Courses","text":"<ul> <li>Time Series Analysis in Python - DataCamp</li> <li>Practical Time Series Analysis - Coursera</li> </ul>"},{"location":"resources/resources-libraries/#books","title":"Books","text":"<ul> <li>\"Forecasting: Principles and Practice\" by Hyndman &amp; Athanasopoulos (Free online)</li> <li>\"Time Series Analysis and Its Applications\" by Shumway &amp; Stoffer</li> </ul>"},{"location":"resources/resources-libraries/#tutorials-blogs","title":"Tutorials &amp; Blogs","text":"<ul> <li>Machine Learning Mastery - Time Series</li> <li>Towards Data Science - Time Series Tag</li> </ul>"},{"location":"resources/resources-libraries/#integration-with-this-guide","title":"\ud83d\udd04 Integration with This Guide","text":"<p>You can combine these libraries with the techniques learned in this guide:</p> <ol> <li>Use this guide to understand fundamentals</li> <li>Apply libraries for production-ready solutions</li> <li>Combine approaches for best results</li> </ol> <p>Example integration: <pre><code># Use our feature engineering\nfrom our_guide import create_lag_features\n\n# Apply to library models\nfeatures = create_lag_features(df)\nmodel = Prophet()\nmodel.add_regressor('rainfall_lag1')\nmodel.fit(features)\n</code></pre></p> <p>Best Practice</p> <p>Start with simple models (Prophet, ARIMA) to establish baselines, then move to complex models (neural networks) only if needed.</p> <p>Data Requirements</p> <p>Most neural network libraries require at least 1000+ observations for good performance.</p> <p>:material-arrow-left: Artificial Neural Network</p> <p>:material-home: Home</p>"},{"location":"setup/data-import/","title":"Data Import &amp; Preparation","text":""},{"location":"setup/data-import/#dataset-structure","title":"\ud83d\udcc2 Dataset Structure","text":"<p>This guide uses a time series dataset containing hydrological and meteorological variables. Let's understand the data structure and learn how to import it properly.</p>"},{"location":"setup/data-import/#expected-data-format","title":"Expected Data Format","text":"<p>Your CSV file should have the following structure:</p> Date Rainfall Tmax Tmin Discharge 1981-01-01 0.0 20.7 8.4 0.528 1981-01-02 12.2 17.9 11.2 0.528 1981-01-03 0.0 18.8 7.8 0.441 ... ... ... ... ... <p>Variable Descriptions</p> <ul> <li>Date: Daily timestamps (YYYY-MM-DD format)</li> <li>Rainfall: Daily precipitation in millimeters (mm)</li> <li>Tmax: Maximum daily temperature in Celsius (\u00b0C)</li> <li>Tmin: Minimum daily temperature in Celsius (\u00b0C)</li> <li>Discharge: Stream discharge in cubic meters per second (m\u00b3/s)</li> </ul>"},{"location":"setup/data-import/#import-required-libraries","title":"\ud83d\udd04 Import Required Libraries","text":"<p>First, let's import all the necessary libraries for our analysis:</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_ccf\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam, SGD\nfrom keras import backend as K\nfrom keras.utils import plot_model\nimport warnings\nimport random\n\n# Set random seeds for reproducibility\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\nrandom.seed(42)\ntf.random.set_seed(42)\n</code></pre> <p>Reproducibility</p> <p>Setting random seeds ensures that your results are reproducible across different runs.</p>"},{"location":"setup/data-import/#loading-the-data","title":"\ud83d\udce5 Loading the Data","text":""},{"location":"setup/data-import/#basic-data-import","title":"Basic Data Import","text":"<pre><code># Load the CSV file\ndf = pd.read_csv('Discharge_30years.csv', \n                 parse_dates=['Date'], \n                 index_col='Date')\n\n# Sort by date (important for time series)\ndf = df.sort_values('Date')\n\n# Display first 5 rows\ndf.head()\n</code></pre>"},{"location":"setup/data-import/#alternative-file-paths","title":"Alternative File Paths","text":"<p>If your file is in a different location:</p> WindowsMac/LinuxGoogle Colab <pre><code>df = pd.read_csv(r'C:\\Users\\YourName\\Documents\\Discharge_30years.csv', \n                 parse_dates=['Date'], \n                 index_col='Date')\n</code></pre> <pre><code>df = pd.read_csv('/home/username/documents/Discharge_30years.csv', \n                 parse_dates=['Date'], \n                 index_col='Date')\n</code></pre> <pre><code>from google.colab import files\nuploaded = files.upload()\ndf = pd.read_csv('Discharge_30years.csv', \n                 parse_dates=['Date'], \n                 index_col='Date')\n</code></pre>"},{"location":"setup/data-import/#data-exploration","title":"\ud83d\udd0d Data Exploration","text":""},{"location":"setup/data-import/#basic-information","title":"Basic Information","text":"<pre><code># Check data shape\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Number of records: {df.shape[0]}\")\nprint(f\"Number of variables: {df.shape[1]}\")\n\n# Data types\nprint(\"\\nData types:\")\nprint(df.dtypes)\n\n# Statistical summary\nprint(\"\\nStatistical Summary:\")\ndf.describe()\n</code></pre>"},{"location":"setup/data-import/#check-for-missing-values","title":"Check for Missing Values","text":"<pre><code># Check missing values\nmissing = df.isnull().sum()\nprint(\"Missing values per column:\")\nprint(missing)\n\n# Visualize missing data\nimport matplotlib.pyplot as plt\n\nif missing.sum() &gt; 0:\n    plt.figure(figsize=(10, 4))\n    missing[missing &gt; 0].plot(kind='bar')\n    plt.title('Missing Values by Column')\n    plt.ylabel('Count')\n    plt.xticks(rotation=45)\n    plt.show()\nelse:\n    print(\"No missing values found!\")\n</code></pre>"},{"location":"setup/data-import/#data-visualization","title":"\ud83d\udcca Data Visualization","text":""},{"location":"setup/data-import/#time-series-plot","title":"Time Series Plot","text":"<pre><code>fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n\n# Rainfall\naxes[0].plot(df.index, df['Rainfall'], color='blue', alpha=0.7)\naxes[0].set_ylabel('Rainfall (mm)')\naxes[0].set_title('30 Years of Hydrological Data')\naxes[0].grid(True, alpha=0.3)\n\n# Temperature\naxes[1].plot(df.index, df['Tmax'], color='red', alpha=0.7, label='Tmax')\naxes[1].plot(df.index, df['Tmin'], color='orange', alpha=0.7, label='Tmin')\naxes[1].set_ylabel('Temperature (\u00b0C)')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Discharge\naxes[2].plot(df.index, df['Discharge'], color='green', alpha=0.7)\naxes[2].set_ylabel('Discharge (m\u00b3/s)')\naxes[2].grid(True, alpha=0.3)\n\n# Rainfall vs Discharge (scatter)\naxes[3].scatter(df['Rainfall'], df['Discharge'], alpha=0.5, s=1)\naxes[3].set_xlabel('Rainfall (mm)')\naxes[3].set_ylabel('Discharge (m\u00b3/s)')\naxes[3].set_title('Rainfall vs Discharge Relationship')\naxes[3].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"setup/data-import/#distribution-analysis","title":"Distribution Analysis","text":"<pre><code>fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# Rainfall distribution\naxes[0, 0].hist(df['Rainfall'], bins=50, edgecolor='black', alpha=0.7)\naxes[0, 0].set_title('Rainfall Distribution')\naxes[0, 0].set_xlabel('Rainfall (mm)')\naxes[0, 0].set_ylabel('Frequency')\n\n# Temperature distribution\naxes[0, 1].hist(df['Tmax'], bins=30, alpha=0.5, label='Tmax', color='red')\naxes[0, 1].hist(df['Tmin'], bins=30, alpha=0.5, label='Tmin', color='blue')\naxes[0, 1].set_title('Temperature Distribution')\naxes[0, 1].set_xlabel('Temperature (\u00b0C)')\naxes[0, 1].legend()\n\n# Discharge distribution\naxes[1, 0].hist(df['Discharge'], bins=50, edgecolor='black', alpha=0.7, color='green')\naxes[1, 0].set_title('Discharge Distribution')\naxes[1, 0].set_xlabel('Discharge (m\u00b3/s)')\n\n# Box plots\naxes[1, 1].boxplot([df['Rainfall'], df['Tmax'], df['Tmin'], df['Discharge']], \n                    labels=['Rainfall', 'Tmax', 'Tmin', 'Discharge'])\naxes[1, 1].set_title('Variable Distributions (Box Plot)')\naxes[1, 1].set_ylabel('Values (normalized for comparison)')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"setup/data-import/#identify-inputs-and-outputs","title":"\ud83c\udfaf Identify Inputs and Outputs","text":""},{"location":"setup/data-import/#for-simple-linear-regression-slr","title":"For Simple Linear Regression (SLR)","text":"<pre><code># SLR uses only one input variable\nX_slr = df[['Rainfall']]  # Input: Rainfall only (DataFrame with shape (n, 1))\ny_slr = df['Discharge']   # Output: Discharge (Series with shape (n,))\n\nprint(f\"SLR Input shape: {X_slr.shape}\")\nprint(f\"SLR Output shape: {y_slr.shape}\")\n</code></pre>"},{"location":"setup/data-import/#for-multiple-linear-regression-mlr","title":"For Multiple Linear Regression (MLR)","text":"<pre><code># MLR uses multiple input variables\n# Method 1: Explicit selection\nX_mlr = df[['Rainfall', 'Tmax', 'Tmin']]  # All inputs\ny_mlr = df['Discharge']                    # Output\n\n# Method 2: Using iloc (by position)\nX_mlr = df.iloc[:, :-1]  # All columns except last\ny_mlr = df.iloc[:, -1]   # Last column\n\nprint(f\"MLR Input shape: {X_mlr.shape}\")\nprint(f\"MLR Output shape: {y_mlr.shape}\")\n</code></pre>"},{"location":"setup/data-import/#flexible-input-selection","title":"Flexible Input Selection","text":"<pre><code>def select_features(df, target_columns, input_selection=\"all\"):\n    \"\"\"\n    Flexible function to select input and output features\n\n    Parameters:\n    -----------\n    df : DataFrame\n        The complete dataset\n    target_columns : list\n        List of column indices for output variables\n    input_selection : str or list or int\n        - \"all\": Use all columns except target\n        - list: Use specific column indices\n        - int: Use first N columns\n\n    Returns:\n    --------\n    X : DataFrame of input features\n    y : Series or DataFrame of output features\n    \"\"\"\n    y = df.iloc[:, target_columns]\n\n    if input_selection == \"all\":\n        input_columns = [i for i in range(df.shape[1]) if i not in target_columns]\n    elif isinstance(input_selection, list):\n        input_columns = input_selection\n    elif isinstance(input_selection, int):\n        input_columns = list(range(input_selection))\n    else:\n        raise ValueError(\"input_selection must be 'all', a list, or an integer\")\n\n    X = df.iloc[:, input_columns]\n\n    return X, y\n\n# Example usage\nX, y = select_features(df, target_columns=[4], input_selection=\"all\")\n</code></pre>"},{"location":"setup/data-import/#data-preprocessing","title":"\ud83d\udd27 Data Preprocessing","text":""},{"location":"setup/data-import/#handle-missing-values","title":"Handle Missing Values","text":"<pre><code># Option 1: Remove rows with missing values\ndf_clean = df.dropna()\n\n# Option 2: Forward fill (use previous value)\ndf_filled = df.fillna(method='ffill')\n\n# Option 3: Interpolation\ndf_interpolated = df.interpolate(method='linear')\n\n# Option 4: Fill with mean (not recommended for time series)\ndf_mean_filled = df.fillna(df.mean())\n</code></pre>"},{"location":"setup/data-import/#remove-outliers-optional","title":"Remove Outliers (Optional)","text":"<pre><code>def remove_outliers(df, column, n_std=3):\n    \"\"\"Remove outliers beyond n standard deviations\"\"\"\n    mean = df[column].mean()\n    std = df[column].std()\n\n    lower_bound = mean - n_std * std\n    upper_bound = mean + n_std * std\n\n    mask = (df[column] &gt;= lower_bound) &amp; (df[column] &lt;= upper_bound)\n    return df[mask]\n\n# Example: Remove discharge outliers\ndf_no_outliers = remove_outliers(df, 'Discharge', n_std=3)\nprint(f\"Removed {len(df) - len(df_no_outliers)} outliers\")\n</code></pre>"},{"location":"setup/data-import/#save-preprocessed-data","title":"\ud83d\udcbe Save Preprocessed Data","text":"<pre><code># Save cleaned data\ndf.to_csv('Discharge_30years_cleaned.csv')\n\n# Save without index\ndf.to_csv('Discharge_30years_cleaned.csv', index=False)\n\n# Save as Excel\ndf.to_excel('Discharge_30years_cleaned.xlsx')\n\n# Save as pickle (preserves data types)\ndf.to_pickle('Discharge_30years_cleaned.pkl')\n</code></pre>"},{"location":"setup/data-import/#data-quality-checklist","title":"\u2705 Data Quality Checklist","text":"<p>Before proceeding to modeling, ensure:</p> <ul> <li>[ ] Data is loaded correctly with proper date parsing</li> <li>[ ] Data is sorted chronologically</li> <li>[ ] Missing values are handled appropriately</li> <li>[ ] Outliers are identified and addressed if necessary</li> <li>[ ] Data types are correct (dates as datetime, numerics as float/int)</li> <li>[ ] Input and output variables are properly separated</li> <li>[ ] Data visualization confirms expected patterns</li> </ul>"},{"location":"setup/data-import/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>Now that your data is properly imported and prepared, you can proceed to:</p> <ol> <li>Performance Metrics - Understanding model evaluation</li> <li>Feature Engineering - Creating lag features</li> <li>Simple Linear Regression - Building your first model</li> </ol> <p>Ready to Model?</p> <p>Your data is now ready for analysis! The next sections will guide you through building predictive models.</p> <p>:material-arrow-left: Installation</p> <p>:material-arrow-right: Performance Metrics</p>"},{"location":"setup/installation/","title":"Installation Guide","text":""},{"location":"setup/installation/#required-libraries","title":"\ud83d\udce6 Required Libraries","text":"<p>This project uses several powerful Python libraries for data analysis, machine learning, and deep learning.</p>"},{"location":"setup/installation/#libraries-overview","title":"Libraries Overview","text":"<ul> <li> <p>:simple-pandas:{ .lg .middle } Pandas</p> <p>For handling and manipulating tabular data using DataFrames</p> </li> <li> <p>:simple-numpy:{ .lg .middle } NumPy</p> <p>Provides fast numerical operations and multi-dimensional arrays</p> </li> <li> <p>:material-chart-line:{ .lg .middle } Matplotlib</p> <p>Used to create static, animated, and interactive plots</p> </li> <li> <p>:simple-scikitlearn:{ .lg .middle } Scikit-learn</p> <p>A machine learning library with tools for modeling and evaluation</p> </li> <li> <p>:material-chart-box:{ .lg .middle } Statsmodels</p> <p>Enables statistical analysis and time series exploration</p> </li> <li> <p>:simple-tensorflow:{ .lg .middle } TensorFlow</p> <p>A powerful library for building and training deep learning models</p> </li> <li> <p>:simple-keras:{ .lg .middle } Keras</p> <p>High-level API within TensorFlow for fast neural network development</p> </li> <li> <p>:material-graph:{ .lg .middle } Pydot &amp; Graphviz</p> <p>Used to visualize neural network architectures (optional)</p> </li> </ul>"},{"location":"setup/installation/#installation-methods","title":"\ud83d\udd27 Installation Methods","text":""},{"location":"setup/installation/#method-1-using-conda-recommended","title":"Method 1: Using Conda (Recommended)","text":"<p>If you have Anaconda or Miniconda installed, this is the most straightforward method:</p> <pre><code>conda install -c conda-forge pandas numpy matplotlib scikit-learn statsmodels tensorflow pydot graphviz -y\n</code></pre> <p>Why Conda?</p> <p>Conda handles complex dependencies better, especially for TensorFlow and its GPU support.</p>"},{"location":"setup/installation/#method-2-using-pip","title":"Method 2: Using pip","text":"<p>For those using standard Python installations:</p> <pre><code># Upgrade pip first\npip install --upgrade pip\n\n# Install core packages\npip install pandas numpy matplotlib scikit-learn statsmodels\n\n# Install TensorFlow (CPU version)\npip install tensorflow\n\n# Optional: For GPU support\npip install tensorflow-gpu\n\n# Optional: For neural network visualization\npip install pydot graphviz\n</code></pre>"},{"location":"setup/installation/#method-3-using-requirements-file","title":"Method 3: Using Requirements File","text":"<p>Create a <code>requirements.txt</code> file:</p> <pre><code>pandas&gt;=1.3.0\nnumpy&gt;=1.21.0\nmatplotlib&gt;=3.4.0\nscikit-learn&gt;=1.0.0\nstatsmodels&gt;=0.13.0\ntensorflow&gt;=2.10.0\npydot&gt;=1.4.0\ngraphviz&gt;=0.20.0\n</code></pre> <p>Then install all at once:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"setup/installation/#setting-up-virtual-environment","title":"\ud83d\udc0d Setting Up Virtual Environment","text":"<p>Best Practice</p> <p>Always use a virtual environment to avoid package conflicts!</p>"},{"location":"setup/installation/#windows","title":"Windows","text":"<pre><code># Create virtual environment\npython -m venv hydro_env\n\n# Activate it\nhydro_env\\Scripts\\activate\n\n# Install packages\npip install -r requirements.txt\n\n# To deactivate when done\ndeactivate\n</code></pre>"},{"location":"setup/installation/#macoslinux","title":"macOS/Linux","text":"<pre><code># Create virtual environment\npython3 -m venv hydro_env\n\n# Activate it\nsource hydro_env/bin/activate\n\n# Install packages\npip install -r requirements.txt\n\n# To deactivate when done\ndeactivate\n</code></pre>"},{"location":"setup/installation/#verify-installation","title":"\u2705 Verify Installation","text":"<p>After installation, verify everything is working:</p> <pre><code>import sys\nprint(f\"Python version: {sys.version}\")\n\n# Test imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport statsmodels\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Print versions\nprint(f\"Pandas: {pd.__version__}\")\nprint(f\"NumPy: {np.__version__}\")\nprint(f\"Scikit-learn: {sklearn.__version__}\")\nprint(f\"TensorFlow: {tf.__version__}\")\nprint(f\"Keras: {keras.__version__}\")\n\n# Test TensorFlow\nprint(f\"TensorFlow GPU Available: {tf.config.list_physical_devices('GPU')}\")\n</code></pre> <p>Expected output: <pre><code>Python version: 3.9.x (or higher)\nPandas: 1.3.x\nNumPy: 1.21.x\nScikit-learn: 1.0.x\nTensorFlow: 2.10.x\nKeras: 2.10.x\nTensorFlow GPU Available: [] (or list of GPUs if available)\n</code></pre></p>"},{"location":"setup/installation/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"setup/installation/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"ImportError: No module named 'tensorflow' <p>Solution: Ensure you've activated your virtual environment and installed TensorFlow: <pre><code>pip install tensorflow\n</code></pre></p> Graphviz not found <p>Solution: Graphviz requires system installation:</p> <p>Windows: Download from Graphviz website</p> <p>macOS:  <pre><code>brew install graphviz\n</code></pre></p> <p>Linux: <pre><code>sudo apt-get install graphviz  # Ubuntu/Debian\nsudo yum install graphviz       # RHEL/CentOS\n</code></pre></p> Memory errors with large datasets <p>Solution: Consider using: - Smaller batch sizes in neural networks - Data chunking with pandas - Google Colab for free GPU access</p>"},{"location":"setup/installation/#alternative-google-colab","title":"\ud83d\udcbb Alternative: Google Colab","text":"<p>If you prefer not to install locally, use Google Colab:</p> <ol> <li>Go to Google Colab</li> <li>Create a new notebook</li> <li>Most libraries are pre-installed</li> <li>For additional packages:    <pre><code>!pip install statsmodels pydot\n</code></pre></li> </ol>"},{"location":"setup/installation/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<p>Now that you have all the required libraries installed, proceed to:</p> <ul> <li>Data Import - Learn how to load and prepare your discharge data</li> <li>Performance Metrics - Understand model evaluation metrics</li> </ul> <p>GPU Support</p> <p>For faster neural network training, consider setting up GPU support: - NVIDIA GPU with CUDA support - Install CUDA toolkit and cuDNN - Install <code>tensorflow-gpu</code> instead of <code>tensorflow</code></p> <p>:material-arrow-left: Home</p> <p>:material-arrow-right: Data Import</p>"}]}